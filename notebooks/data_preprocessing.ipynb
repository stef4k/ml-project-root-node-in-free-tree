{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f450335",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "This notebook describes the preprocessing steps and a quick exploratory analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c68304a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import ast\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1054a29b",
   "metadata": {},
   "source": [
    "Loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3682e2ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>sentence</th>\n",
       "      <th>n</th>\n",
       "      <th>edgelist</th>\n",
       "      <th>root</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>[(6, 4), (2, 6), (2, 23), (20, 2), (15, 20), (...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>[(8, 9), (14, 8), (4, 14), (5, 4), (1, 2), (6,...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>[(2, 10), (2, 14), (4, 2), (16, 4), (6, 16), (...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>[(30, 1), (14, 24), (21, 14), (3, 21), (7, 3),...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>[(19, 13), (16, 19), (2, 16), (4, 10), (4, 15)...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   language  sentence   n                                           edgelist  \\\n",
       "0  Japanese         2  23  [(6, 4), (2, 6), (2, 23), (20, 2), (15, 20), (...   \n",
       "1  Japanese         5  18  [(8, 9), (14, 8), (4, 14), (5, 4), (1, 2), (6,...   \n",
       "2  Japanese         8  33  [(2, 10), (2, 14), (4, 2), (16, 4), (6, 16), (...   \n",
       "3  Japanese        11  30  [(30, 1), (14, 24), (21, 14), (3, 21), (7, 3),...   \n",
       "4  Japanese        12  19  [(19, 13), (16, 19), (2, 16), (4, 10), (4, 15)...   \n",
       "\n",
       "   root  \n",
       "0    10  \n",
       "1    10  \n",
       "2     3  \n",
       "3    30  \n",
       "4    11  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6ee7e8",
   "metadata": {},
   "source": [
    "Next we check the types and nulls values per column of the training data. No null values are found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0559da58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10500 entries, 0 to 10499\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   language  10500 non-null  object\n",
      " 1   sentence  10500 non-null  int64 \n",
      " 2   n         10500 non-null  int64 \n",
      " 3   edgelist  10500 non-null  object\n",
      " 4   root      10500 non-null  int64 \n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 410.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0024cf",
   "metadata": {},
   "source": [
    "Now we can check some basic statistics of the numeric columns.\n",
    "- On average each sentence has $18.8$ words\n",
    "- The root predicted node can be from the 1st till the 68th node (depending the size of the sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baf4a3f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>n</th>\n",
       "      <th>root</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10500.000000</td>\n",
       "      <td>10500.000000</td>\n",
       "      <td>10500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>494.778000</td>\n",
       "      <td>18.807524</td>\n",
       "      <td>9.844476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>290.256632</td>\n",
       "      <td>8.190593</td>\n",
       "      <td>7.207740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>233.500000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>483.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>742.250000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>995.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>68.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           sentence             n          root\n",
       "count  10500.000000  10500.000000  10500.000000\n",
       "mean     494.778000     18.807524      9.844476\n",
       "std      290.256632      8.190593      7.207740\n",
       "min        2.000000      3.000000      1.000000\n",
       "25%      233.500000     13.000000      4.000000\n",
       "50%      483.000000     18.000000      8.000000\n",
       "75%      742.250000     23.000000     14.000000\n",
       "max      995.000000     70.000000     68.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658ba3b8",
   "metadata": {},
   "source": [
    "In the training data we can find 21 languages with 500 sentences for each language, producing $10,500$ total rows of the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "990245d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total languages: 21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "language  \n",
       "Arabic        500\n",
       "Chinese       500\n",
       "Czech         500\n",
       "English       500\n",
       "Finnish       500\n",
       "French        500\n",
       "Galician      500\n",
       "German        500\n",
       "Hindi         500\n",
       "Icelandic     500\n",
       "Indonesian    500\n",
       "Italian       500\n",
       "Japanese      500\n",
       "Korean        500\n",
       "Polish        500\n",
       "Portuguese    500\n",
       "Russian       500\n",
       "Spanish       500\n",
       "Swedish       500\n",
       "Thai          500\n",
       "Turkish       500\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Total languages: ' + str(data[['language']].value_counts().shape[0]))\n",
    "data[['language']].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ff0f76",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "Now the dataset will be transformed into a training set suitable for binary classification models using centralities as\n",
    "vertex features\n",
    "\n",
    "A function is created that transform an edge list into a networkx graph calculating the centralities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a771247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def centralities(edgelist):\n",
    "    T = nx.from_edgelist(edgelist)\n",
    "    degree = nx.degree_centrality(T)\n",
    "    closeness = nx.closeness_centrality(T)\n",
    "    harmonic = nx.harmonic_centrality(T)\n",
    "    betweeness = nx.betweenness_centrality(T,seed=1)\n",
    "    load = nx.load_centrality(T)\n",
    "    pagerank = nx.pagerank(T)\n",
    "    eigenvector = nx.eigenvector_centrality_numpy(T)\n",
    "    katz = nx.katz_centrality_numpy(T)\n",
    "    information = nx.information_centrality(T)\n",
    "    current_flow_betweenness = nx.current_flow_betweenness_centrality(T)\n",
    "    percolation = nx.percolation_centrality(T)\n",
    "    second_order = nx.second_order_centrality(T)\n",
    "    laplacian = nx.laplacian_centrality(T)\n",
    "    return {v: (degree[v], closeness[v], harmonic[v], betweeness[v],\n",
    "                load[v], pagerank[v], eigenvector[v], katz[v], information[v],\n",
    "                current_flow_betweenness[v], percolation[v],\n",
    "                second_order[v], laplacian[v]) for v in T}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f46aaf",
   "metadata": {},
   "source": [
    "Now we iterate over each row of the training data, transforming it into a graph and calculating the centralities for every node. The new binary dataset contains the features of `language`, `sentence`,`n`, `vertex`, the various centrality scores and `is_root` (which takes values of 0 or 1 if the particular node is a root node). Since we lose all the information regarding the edges, we decided to use as many centrality measures as possible to retain as much of the semantic information as possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38f1f89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>sentence</th>\n",
       "      <th>vertex</th>\n",
       "      <th>n</th>\n",
       "      <th>degree</th>\n",
       "      <th>closeness</th>\n",
       "      <th>harmonic</th>\n",
       "      <th>betweeness</th>\n",
       "      <th>load</th>\n",
       "      <th>pagerank</th>\n",
       "      <th>eigenvector</th>\n",
       "      <th>katz</th>\n",
       "      <th>information</th>\n",
       "      <th>current_flow_betweeness</th>\n",
       "      <th>percolation</th>\n",
       "      <th>second_order</th>\n",
       "      <th>laplacian</th>\n",
       "      <th>is_root</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.159420</td>\n",
       "      <td>5.823846</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.048565</td>\n",
       "      <td>0.149505</td>\n",
       "      <td>0.209086</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>98.762341</td>\n",
       "      <td>0.101449</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.138365</td>\n",
       "      <td>4.561122</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027162</td>\n",
       "      <td>0.068517</td>\n",
       "      <td>0.188298</td>\n",
       "      <td>0.006289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>112.481110</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.184874</td>\n",
       "      <td>6.991703</td>\n",
       "      <td>0.255411</td>\n",
       "      <td>0.255411</td>\n",
       "      <td>0.066901</td>\n",
       "      <td>0.257706</td>\n",
       "      <td>0.228660</td>\n",
       "      <td>0.008403</td>\n",
       "      <td>0.255411</td>\n",
       "      <td>0.255411</td>\n",
       "      <td>84.451169</td>\n",
       "      <td>0.159420</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>5.157179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025477</td>\n",
       "      <td>0.118104</td>\n",
       "      <td>0.190256</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.149888</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.211538</td>\n",
       "      <td>7.146825</td>\n",
       "      <td>0.311688</td>\n",
       "      <td>0.311688</td>\n",
       "      <td>0.042552</td>\n",
       "      <td>0.294710</td>\n",
       "      <td>0.213357</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.311688</td>\n",
       "      <td>0.311688</td>\n",
       "      <td>71.147734</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   language  sentence  vertex   n    degree  closeness  harmonic  betweeness  \\\n",
       "0  Japanese         2       6  23  0.090909   0.159420  5.823846    0.090909   \n",
       "1  Japanese         2       4  23  0.045455   0.138365  4.561122    0.000000   \n",
       "2  Japanese         2       2  23  0.136364   0.184874  6.991703    0.255411   \n",
       "3  Japanese         2      23  23  0.045455   0.157143  5.157179    0.000000   \n",
       "4  Japanese         2      20  23  0.090909   0.211538  7.146825    0.311688   \n",
       "\n",
       "       load  pagerank  eigenvector      katz  information  \\\n",
       "0  0.090909  0.048565     0.149505  0.209086     0.007246   \n",
       "1  0.000000  0.027162     0.068517  0.188298     0.006289   \n",
       "2  0.255411  0.066901     0.257706  0.228660     0.008403   \n",
       "3  0.000000  0.025477     0.118104  0.190256     0.007143   \n",
       "4  0.311688  0.042552     0.294710  0.213357     0.009615   \n",
       "\n",
       "   current_flow_betweeness  percolation  second_order  laplacian  is_root  \n",
       "0                 0.090909     0.090909     98.762341   0.101449        0  \n",
       "1                 0.000000     0.000000    112.481110   0.043478        0  \n",
       "2                 0.255411     0.255411     84.451169   0.159420        0  \n",
       "3                 0.000000     0.000000    100.149888   0.057971        0  \n",
       "4                 0.311688     0.311688     71.147734   0.130435        0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['language', 'sentence', 'vertex', 'n',\n",
    "           'degree', 'closeness', 'harmonic', 'betweeness', 'load', 'pagerank',\n",
    "           'eigenvector', 'katz', 'information', 'current_flow_betweeness',\n",
    "           'percolation', 'second_order', 'laplacian', 'is_root']\n",
    "binary_data_list = []\n",
    "# Saving column of edges as a list instead of a string\n",
    "data['edgelist'] = data['edgelist'].apply(ast.literal_eval)\n",
    "\n",
    "for row in data.itertuples(index=False):\n",
    "    for node, (degree, closeness, harmonic, betweeness, load,\n",
    "              pagerank, eigenvector, katz, information, current_flow_betweeness,\n",
    "              percolation, second_order, laplacian) in centralities(row.edgelist).items():\n",
    "        new_row = {'language': row.language,\n",
    "                   'sentence': row.sentence,\n",
    "                   'vertex': node,\n",
    "                   'n': row.n,\n",
    "                   'degree': degree,\n",
    "                   'closeness': closeness,\n",
    "                   'harmonic': harmonic,\n",
    "                   'betweeness': betweeness,\n",
    "                   'load': load,\n",
    "                   'pagerank': pagerank,\n",
    "                   'eigenvector': eigenvector,\n",
    "                   'katz': katz,\n",
    "                   'information': information,\n",
    "                   'current_flow_betweeness': current_flow_betweeness,\n",
    "                   'percolation': percolation,\n",
    "                   'second_order': second_order,\n",
    "                   'laplacian': laplacian,\n",
    "                   'is_root': 1 if node == row.root else 0}\n",
    "        binary_data_list.append(new_row)\n",
    "\n",
    "expanded_data = pd.DataFrame(binary_data_list, columns=columns)\n",
    "expanded_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3220e61b",
   "metadata": {},
   "source": [
    "Next we can normalize the centrality measures of training data but within the same sentence group of the same language:\n",
    "- Different languages mean different trees (in word order or number of words) and thus centrality values can't be compared directly across languages\n",
    "- Different sentences also mean different trees\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1639738c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stef4\\AppData\\Local\\Temp\\ipykernel_18464\\2028615403.py:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  normalized_expanded_data = expanded_data.groupby(['language', 'sentence'], group_keys=False).apply(scale_within_sentence).reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>sentence</th>\n",
       "      <th>vertex</th>\n",
       "      <th>n</th>\n",
       "      <th>degree</th>\n",
       "      <th>closeness</th>\n",
       "      <th>harmonic</th>\n",
       "      <th>betweeness</th>\n",
       "      <th>load</th>\n",
       "      <th>pagerank</th>\n",
       "      <th>eigenvector</th>\n",
       "      <th>katz</th>\n",
       "      <th>information</th>\n",
       "      <th>current_flow_betweeness</th>\n",
       "      <th>percolation</th>\n",
       "      <th>second_order</th>\n",
       "      <th>laplacian</th>\n",
       "      <th>is_root</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>0.133038</td>\n",
       "      <td>-0.558160</td>\n",
       "      <td>-0.308023</td>\n",
       "      <td>-0.676962</td>\n",
       "      <td>-0.676962</td>\n",
       "      <td>0.409538</td>\n",
       "      <td>-0.327923</td>\n",
       "      <td>0.074694</td>\n",
       "      <td>-0.558160</td>\n",
       "      <td>-0.676962</td>\n",
       "      <td>-0.676962</td>\n",
       "      <td>0.489782</td>\n",
       "      <td>-0.031273</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>-1.396899</td>\n",
       "      <td>-1.111953</td>\n",
       "      <td>-1.442852</td>\n",
       "      <td>-1.126101</td>\n",
       "      <td>-1.126101</td>\n",
       "      <td>-1.313638</td>\n",
       "      <td>-1.131636</td>\n",
       "      <td>-1.460142</td>\n",
       "      <td>-1.111953</td>\n",
       "      <td>-1.126101</td>\n",
       "      <td>-1.126101</td>\n",
       "      <td>1.149235</td>\n",
       "      <td>-1.469827</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>1.662975</td>\n",
       "      <td>0.111311</td>\n",
       "      <td>0.741550</td>\n",
       "      <td>0.135764</td>\n",
       "      <td>0.135764</td>\n",
       "      <td>1.885752</td>\n",
       "      <td>0.745846</td>\n",
       "      <td>1.519954</td>\n",
       "      <td>0.111311</td>\n",
       "      <td>0.135764</td>\n",
       "      <td>0.135764</td>\n",
       "      <td>-0.198147</td>\n",
       "      <td>1.407281</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>-1.396899</td>\n",
       "      <td>-0.618060</td>\n",
       "      <td>-0.907167</td>\n",
       "      <td>-1.126101</td>\n",
       "      <td>-1.126101</td>\n",
       "      <td>-1.449223</td>\n",
       "      <td>-0.639537</td>\n",
       "      <td>-1.315616</td>\n",
       "      <td>-0.618060</td>\n",
       "      <td>-1.126101</td>\n",
       "      <td>-1.126101</td>\n",
       "      <td>0.556481</td>\n",
       "      <td>-1.110188</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>0.133038</td>\n",
       "      <td>0.812630</td>\n",
       "      <td>0.880961</td>\n",
       "      <td>0.413803</td>\n",
       "      <td>0.413803</td>\n",
       "      <td>-0.074579</td>\n",
       "      <td>1.113070</td>\n",
       "      <td>0.390094</td>\n",
       "      <td>0.812630</td>\n",
       "      <td>0.413803</td>\n",
       "      <td>0.413803</td>\n",
       "      <td>-0.837635</td>\n",
       "      <td>0.688004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   language  sentence  vertex   n    degree  closeness  harmonic  betweeness  \\\n",
       "0  Japanese         2       6  23  0.133038  -0.558160 -0.308023   -0.676962   \n",
       "1  Japanese         2       4  23 -1.396899  -1.111953 -1.442852   -1.126101   \n",
       "2  Japanese         2       2  23  1.662975   0.111311  0.741550    0.135764   \n",
       "3  Japanese         2      23  23 -1.396899  -0.618060 -0.907167   -1.126101   \n",
       "4  Japanese         2      20  23  0.133038   0.812630  0.880961    0.413803   \n",
       "\n",
       "       load  pagerank  eigenvector      katz  information  \\\n",
       "0 -0.676962  0.409538    -0.327923  0.074694    -0.558160   \n",
       "1 -1.126101 -1.313638    -1.131636 -1.460142    -1.111953   \n",
       "2  0.135764  1.885752     0.745846  1.519954     0.111311   \n",
       "3 -1.126101 -1.449223    -0.639537 -1.315616    -0.618060   \n",
       "4  0.413803 -0.074579     1.113070  0.390094     0.812630   \n",
       "\n",
       "   current_flow_betweeness  percolation  second_order  laplacian  is_root  \n",
       "0                -0.676962    -0.676962      0.489782  -0.031273        0  \n",
       "1                -1.126101    -1.126101      1.149235  -1.469827        0  \n",
       "2                 0.135764     0.135764     -0.198147   1.407281        0  \n",
       "3                -1.126101    -1.126101      0.556481  -1.110188        0  \n",
       "4                 0.413803     0.413803     -0.837635   0.688004        0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centrality_cols = [\n",
    "    'degree', 'closeness', 'harmonic', 'betweeness', 'load', 'pagerank', \n",
    "    'eigenvector', 'katz', 'information', 'current_flow_betweeness', \n",
    "    'percolation', 'second_order', 'laplacian'\n",
    "]\n",
    "\n",
    "# Function to apply StandardScaler in each sentence & language group\n",
    "def scale_within_sentence(group):\n",
    "    scaler = StandardScaler()\n",
    "    group[centrality_cols] = scaler.fit_transform(group[centrality_cols])\n",
    "    return group\n",
    "\n",
    "normalized_expanded_data = expanded_data.groupby(['language', 'sentence'], group_keys=False).apply(scale_within_sentence).reset_index(drop=True)\n",
    "normalized_expanded_data.to_csv('../data/normalized_expanded_train.csv')\n",
    "normalized_expanded_data.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python ML-project(myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
