{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a55429ae",
   "metadata": {},
   "source": [
    "# Non-Linear Approach\n",
    "This attempt focuses on using normalized centrality measures and a random forest for each of the languages\n",
    "\n",
    "Starting by importing the right libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c508369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit, StratifiedGroupKFold\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import statistics\n",
    "import random\n",
    "import warnings\n",
    "from sklearn.exceptions import FitFailedWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FitFailedWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82005bf0",
   "metadata": {},
   "source": [
    "Next we read the train data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f0da078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>sentence</th>\n",
       "      <th>vertex</th>\n",
       "      <th>n</th>\n",
       "      <th>degree</th>\n",
       "      <th>closeness</th>\n",
       "      <th>harmonic</th>\n",
       "      <th>betweeness</th>\n",
       "      <th>load</th>\n",
       "      <th>pagerank</th>\n",
       "      <th>eigenvector</th>\n",
       "      <th>katz</th>\n",
       "      <th>information</th>\n",
       "      <th>current_flow_betweeness</th>\n",
       "      <th>percolation</th>\n",
       "      <th>second_order</th>\n",
       "      <th>laplacian</th>\n",
       "      <th>is_root</th>\n",
       "      <th>is_leaf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.159420</td>\n",
       "      <td>5.823846</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.048565</td>\n",
       "      <td>0.149505</td>\n",
       "      <td>0.209086</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>98.762341</td>\n",
       "      <td>0.101449</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.138365</td>\n",
       "      <td>4.561122</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027162</td>\n",
       "      <td>0.068517</td>\n",
       "      <td>0.188298</td>\n",
       "      <td>0.006289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>112.481110</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.184874</td>\n",
       "      <td>6.991703</td>\n",
       "      <td>0.255411</td>\n",
       "      <td>0.255411</td>\n",
       "      <td>0.066901</td>\n",
       "      <td>0.257706</td>\n",
       "      <td>0.228660</td>\n",
       "      <td>0.008403</td>\n",
       "      <td>0.255411</td>\n",
       "      <td>0.255411</td>\n",
       "      <td>84.451169</td>\n",
       "      <td>0.159420</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>5.157179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025477</td>\n",
       "      <td>0.118104</td>\n",
       "      <td>0.190256</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.149888</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.211538</td>\n",
       "      <td>7.146825</td>\n",
       "      <td>0.311688</td>\n",
       "      <td>0.311688</td>\n",
       "      <td>0.042552</td>\n",
       "      <td>0.294710</td>\n",
       "      <td>0.213357</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.311688</td>\n",
       "      <td>0.311688</td>\n",
       "      <td>71.147734</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   language  sentence  vertex   n    degree  closeness  harmonic  betweeness  \\\n",
       "0  Japanese         2       6  23  0.090909   0.159420  5.823846    0.090909   \n",
       "1  Japanese         2       4  23  0.045455   0.138365  4.561122    0.000000   \n",
       "2  Japanese         2       2  23  0.136364   0.184874  6.991703    0.255411   \n",
       "3  Japanese         2      23  23  0.045455   0.157143  5.157179    0.000000   \n",
       "4  Japanese         2      20  23  0.090909   0.211538  7.146825    0.311688   \n",
       "\n",
       "       load  pagerank  eigenvector      katz  information  \\\n",
       "0  0.090909  0.048565     0.149505  0.209086     0.007246   \n",
       "1  0.000000  0.027162     0.068517  0.188298     0.006289   \n",
       "2  0.255411  0.066901     0.257706  0.228660     0.008403   \n",
       "3  0.000000  0.025477     0.118104  0.190256     0.007143   \n",
       "4  0.311688  0.042552     0.294710  0.213357     0.009615   \n",
       "\n",
       "   current_flow_betweeness  percolation  second_order  laplacian  is_root  \\\n",
       "0                 0.090909     0.090909     98.762341   0.101449        0   \n",
       "1                 0.000000     0.000000    112.481110   0.043478        0   \n",
       "2                 0.255411     0.255411     84.451169   0.159420        0   \n",
       "3                 0.000000     0.000000    100.149888   0.057971        0   \n",
       "4                 0.311688     0.311688     71.147734   0.130435        0   \n",
       "\n",
       "   is_leaf  \n",
       "0        0  \n",
       "1        1  \n",
       "2        0  \n",
       "3        1  \n",
       "4        0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/expanded_train_with_leaf.csv')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e921a9ca",
   "metadata": {},
   "source": [
    "Now we will focus only in one language, opting to go for `Polish`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83483938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>sentence</th>\n",
       "      <th>vertex</th>\n",
       "      <th>n</th>\n",
       "      <th>degree</th>\n",
       "      <th>closeness</th>\n",
       "      <th>harmonic</th>\n",
       "      <th>betweeness</th>\n",
       "      <th>load</th>\n",
       "      <th>pagerank</th>\n",
       "      <th>eigenvector</th>\n",
       "      <th>katz</th>\n",
       "      <th>information</th>\n",
       "      <th>current_flow_betweeness</th>\n",
       "      <th>percolation</th>\n",
       "      <th>second_order</th>\n",
       "      <th>laplacian</th>\n",
       "      <th>is_root</th>\n",
       "      <th>is_leaf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>116304</th>\n",
       "      <td>German</td>\n",
       "      <td>797</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.306122</td>\n",
       "      <td>6.902381</td>\n",
       "      <td>0.447619</td>\n",
       "      <td>0.447619</td>\n",
       "      <td>0.093527</td>\n",
       "      <td>0.441785</td>\n",
       "      <td>0.279043</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>4.476190e-01</td>\n",
       "      <td>0.447619</td>\n",
       "      <td>44.631827</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115825</th>\n",
       "      <td>German</td>\n",
       "      <td>733</td>\n",
       "      <td>9</td>\n",
       "      <td>24</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.264368</td>\n",
       "      <td>7.416667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022941</td>\n",
       "      <td>0.191661</td>\n",
       "      <td>0.187486</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>91.858587</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112469</th>\n",
       "      <td>German</td>\n",
       "      <td>397</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.162162</td>\n",
       "      <td>4.145238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033421</td>\n",
       "      <td>0.022759</td>\n",
       "      <td>0.205355</td>\n",
       "      <td>0.009009</td>\n",
       "      <td>2.670340e-16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>103.932671</td>\n",
       "      <td>0.048387</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114550</th>\n",
       "      <td>German</td>\n",
       "      <td>618</td>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>7.650000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.050601</td>\n",
       "      <td>0.196587</td>\n",
       "      <td>0.218293</td>\n",
       "      <td>0.015385</td>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>75.299402</td>\n",
       "      <td>0.109589</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114158</th>\n",
       "      <td>German</td>\n",
       "      <td>567</td>\n",
       "      <td>11</td>\n",
       "      <td>37</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.208092</td>\n",
       "      <td>9.046429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014849</td>\n",
       "      <td>0.066178</td>\n",
       "      <td>0.147068</td>\n",
       "      <td>0.005780</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>184.412581</td>\n",
       "      <td>0.030075</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       language  sentence  vertex   n    degree  closeness  harmonic  \\\n",
       "116304   German       797      13  16  0.200000   0.306122  6.902381   \n",
       "115825   German       733       9  24  0.043478   0.264368  7.416667   \n",
       "112469   German       397       5  19  0.055556   0.162162  4.145238   \n",
       "114550   German       618      15  21  0.100000   0.307692  7.650000   \n",
       "114158   German       567      11  37  0.027778   0.208092  9.046429   \n",
       "\n",
       "        betweeness      load  pagerank  eigenvector      katz  information  \\\n",
       "116304    0.447619  0.447619  0.093527     0.441785  0.279043     0.020408   \n",
       "115825    0.000000  0.000000  0.022941     0.191661  0.187486     0.011494   \n",
       "112469    0.000000  0.000000  0.033421     0.022759  0.205355     0.009009   \n",
       "114550    0.100000  0.100000  0.050601     0.196587  0.218293     0.015385   \n",
       "114158    0.000000  0.000000  0.014849     0.066178  0.147068     0.005780   \n",
       "\n",
       "        current_flow_betweeness  percolation  second_order  laplacian  \\\n",
       "116304             4.476190e-01     0.447619     44.631827   0.265306   \n",
       "115825             0.000000e+00     0.000000     91.858587   0.071429   \n",
       "112469             2.670340e-16     0.000000    103.932671   0.048387   \n",
       "114550             1.000000e-01     0.100000     75.299402   0.109589   \n",
       "114158             0.000000e+00     0.000000    184.412581   0.030075   \n",
       "\n",
       "        is_root  is_leaf  \n",
       "116304        0        0  \n",
       "115825        0        1  \n",
       "112469        0        1  \n",
       "114550        0        0  \n",
       "114158        0        1  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_data = (\n",
    "    data\n",
    "    [ (data.language == 'German') ]  # only non-leaf nodes\n",
    "    .sample(frac=1, random_state=1)\n",
    "    .copy()\n",
    ")\n",
    "german_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f4a227",
   "metadata": {},
   "source": [
    "Now we can split the data in train and test in a way to keep the same sentences (sentence level and node level of sentence) in the same group. Each sentence and its nodes will only be found in train or validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e7693d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = german_data.copy()#.drop(['is_root','language','n'], axis=1).copy()\n",
    "y = german_data['is_root'].copy()\n",
    "\n",
    "groups = german_data['sentence']\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "X_train, X_val= X.iloc[train_idx], X.iloc[val_idx]\n",
    "y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c84185",
   "metadata": {},
   "source": [
    "Now we also define the cross validation strategy. We also split the training data in 5 different folds ensuring that data of of the same sentence can not be found in different folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c45bd94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedGroupKFold(n_splits=5)\n",
    "groups = X_train['sentence']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaec98e",
   "metadata": {},
   "source": [
    "Next we can define the logistic regression model parameters that we want to try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "af856faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use an imbalanced‐aware RF\n",
    "model = BalancedRandomForestClassifier(\n",
    "    n_jobs=8,\n",
    "    random_state=2,\n",
    "    sampling_strategy='auto'   # balances classes by undersampling the majority\n",
    ")\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc584422",
   "metadata": {},
   "source": [
    "We use a custom scoring method that picks one root per sentence. For each sentence, it chooses the node with the highest chance of being the root (class 1). The final score is just the percentage of sentences where we picked the correct root.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3bbde6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_prediction_score(estimator, X, y_true):\n",
    "    \"\"\"\n",
    "    Scoring function that extracts sentence IDs from X and computes\n",
    "    root prediction accuracy per sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    sentence_ids = X['sentence'].values\n",
    "    X_features = X.copy()\n",
    "    # Predict probabilities\n",
    "    probs = estimator.predict_proba(X_features)[:, 1]\n",
    "    # Build DataFrame for groupby\n",
    "    df_pred = pd.DataFrame({\n",
    "        'sentence': sentence_ids,\n",
    "        'is_root': y_true,\n",
    "        'root_prob': probs\n",
    "    })\n",
    "\n",
    "    predicted_roots = df_pred.loc[df_pred.groupby('sentence')['root_prob'].idxmax()]\n",
    "    accuracy = float((predicted_roots['is_root'] == 1).mean())\n",
    "    return accuracy\n",
    "\n",
    "root_scorer = make_scorer(root_prediction_score, greater_is_better=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b790f57",
   "metadata": {},
   "source": [
    "Now we can do the gridsearch, combining defined parameters, the custom scoring function, the cross validation strategy (that ensures no data leakage):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aae5bce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 10}\n",
      "Best score: 0.29\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_cols = [\n",
    "    'sentence', 'degree', 'closeness', 'harmonic', 'betweeness', 'load', 'pagerank',\n",
    "    'eigenvector', 'katz', 'information', 'current_flow_betweeness',\n",
    "    'percolation', 'second_order', 'laplacian'\n",
    "]\n",
    "\n",
    "grid_search = RandomizedSearchCV(n_iter=200,\n",
    "    estimator=model,\n",
    "    param_distributions=param_grid,\n",
    "    cv=cv,\n",
    "    scoring=root_prediction_score,\n",
    "    n_jobs=8,\n",
    "    random_state=2\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train[feature_cols], y_train, groups=groups)\n",
    "\n",
    "print(\"Best params:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104a3cd3",
   "metadata": {},
   "source": [
    "We will use the best estimator found to see the performance in the validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "df451647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_estimator = grid_search.best_estimator_\n",
    "root_prediction_score(best_estimator, X_val[feature_cols], y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b346f0ab",
   "metadata": {},
   "source": [
    "### Extending all languages\n",
    "Next we create a script that follows the previous steps for each of the 21 languages. As a result we will have one logistic regression model for each language. To predict the test data, we will use the particular logistic regression depending on the language of the row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce52cf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Japanese\n",
      "Grid search score: 0.12000000000000002\n",
      "Validation score: 0.11\n",
      "{'n_estimators': 400, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 10, 'criterion': 'gini', 'bootstrap': True}\n",
      "---------------------------------------\n",
      "Finnish\n",
      "Grid search score: 0.34750000000000003\n",
      "Validation score: 0.36\n",
      "{'warm_start': True, 'solver': 'lbfgs', 'penalty': 'l2', 'max_iter': 5000, 'l1_ratio': 0.5, 'fit_intercept': True, 'C': 100}\n",
      "---------------------------------------\n",
      "Galician\n",
      "Grid search score: 0.28500000000000003\n",
      "Validation score: 0.3\n",
      "{'warm_start': True, 'solver': 'lbfgs', 'penalty': None, 'max_iter': 5000, 'l1_ratio': 0.5, 'fit_intercept': False, 'C': 100}\n",
      "---------------------------------------\n",
      "English\n",
      "Grid search score: 0.29250000000000004\n",
      "Validation score: 0.28\n",
      "{'warm_start': True, 'solver': 'lbfgs', 'penalty': None, 'max_iter': 5000, 'l1_ratio': 1, 'fit_intercept': True, 'C': 100}\n",
      "---------------------------------------\n",
      "Hindi\n",
      "Grid search score: 0.2125\n",
      "Validation score: 0.27\n",
      "{'warm_start': True, 'solver': 'lbfgs', 'penalty': 'l2', 'max_iter': 2500, 'l1_ratio': 0.5, 'fit_intercept': True, 'C': 1}\n",
      "---------------------------------------\n",
      "French\n",
      "Grid search score: 0.26749999999999996\n",
      "Validation score: 0.29\n",
      "{'warm_start': True, 'solver': 'lbfgs', 'penalty': 'l2', 'max_iter': 5000, 'l1_ratio': 1, 'fit_intercept': True, 'C': 1}\n",
      "---------------------------------------\n",
      "Italian\n",
      "Grid search score: 0.27749999999999997\n",
      "Validation score: 0.2\n",
      "{'warm_start': True, 'solver': 'lbfgs', 'penalty': None, 'max_iter': 2500, 'l1_ratio': 0, 'fit_intercept': False, 'C': 100}\n",
      "---------------------------------------\n",
      "Indonesian\n",
      "Grid search score: 0.3075\n",
      "Validation score: 0.28\n",
      "{'warm_start': True, 'solver': 'lbfgs', 'penalty': None, 'max_iter': 1000, 'l1_ratio': 1, 'fit_intercept': False, 'C': 0.01}\n",
      "---------------------------------------\n",
      "Swedish\n",
      "Grid search score: 0.3125\n",
      "Validation score: 0.34\n",
      "{'warm_start': True, 'solver': 'sag', 'penalty': None, 'max_iter': 2500, 'l1_ratio': 0.5, 'fit_intercept': False, 'C': 1}\n",
      "---------------------------------------\n",
      "Spanish\n",
      "Grid search score: 0.315\n",
      "Validation score: 0.32\n",
      "{'warm_start': True, 'solver': 'sag', 'penalty': 'l2', 'max_iter': 5000, 'l1_ratio': 0.5, 'fit_intercept': False, 'C': 10}\n",
      "---------------------------------------\n",
      "Icelandic\n",
      "Grid search score: 0.3325\n",
      "Validation score: 0.33\n",
      "{'warm_start': False, 'solver': 'sag', 'penalty': 'l2', 'max_iter': 5000, 'l1_ratio': 1, 'fit_intercept': False, 'C': 1}\n",
      "---------------------------------------\n",
      "German\n",
      "Grid search score: 0.27499999999999997\n",
      "Validation score: 0.31\n",
      "{'warm_start': True, 'solver': 'lbfgs', 'penalty': None, 'max_iter': 1000, 'l1_ratio': 0.5, 'fit_intercept': False, 'C': 100}\n",
      "---------------------------------------\n",
      "Korean\n",
      "Grid search score: 0.3125\n",
      "Validation score: 0.34\n",
      "{'warm_start': True, 'solver': 'sag', 'penalty': 'l2', 'max_iter': 5000, 'l1_ratio': 0, 'fit_intercept': False, 'C': 100}\n",
      "---------------------------------------\n",
      "Polish\n",
      "Grid search score: 0.27\n",
      "Validation score: 0.32\n",
      "{'warm_start': True, 'solver': 'saga', 'penalty': 'l1', 'max_iter': 1000, 'l1_ratio': 1, 'fit_intercept': False, 'C': 10}\n",
      "---------------------------------------\n",
      "Thai\n",
      "Grid search score: 0.24\n",
      "Validation score: 0.3\n",
      "{'warm_start': True, 'solver': 'saga', 'penalty': 'l1', 'max_iter': 2500, 'l1_ratio': 0.5, 'fit_intercept': True, 'C': 0.01}\n",
      "---------------------------------------\n",
      "Turkish\n",
      "Grid search score: 0.3\n",
      "Validation score: 0.36\n",
      "{'warm_start': False, 'solver': 'saga', 'penalty': 'l2', 'max_iter': 1000, 'l1_ratio': 0.5, 'fit_intercept': True, 'C': 10}\n",
      "---------------------------------------\n",
      "Czech\n",
      "Grid search score: 0.27\n",
      "Validation score: 0.32\n",
      "{'warm_start': False, 'solver': 'saga', 'penalty': None, 'max_iter': 1000, 'l1_ratio': 0, 'fit_intercept': True, 'C': 0.01}\n",
      "---------------------------------------\n",
      "Chinese\n",
      "Grid search score: 0.2975\n",
      "Validation score: 0.26\n",
      "{'warm_start': True, 'solver': 'lbfgs', 'penalty': None, 'max_iter': 2500, 'l1_ratio': 0.5, 'fit_intercept': True, 'C': 10}\n",
      "---------------------------------------\n",
      "Portuguese\n",
      "Grid search score: 0.32\n",
      "Validation score: 0.28\n",
      "{'warm_start': False, 'solver': 'lbfgs', 'penalty': None, 'max_iter': 1000, 'l1_ratio': 0, 'fit_intercept': False, 'C': 0.1}\n",
      "---------------------------------------\n",
      "Arabic\n",
      "Grid search score: 0.28\n",
      "Validation score: 0.33\n",
      "{'warm_start': True, 'solver': 'saga', 'penalty': 'l2', 'max_iter': 1000, 'l1_ratio': 1, 'fit_intercept': True, 'C': 1}\n",
      "---------------------------------------\n",
      "Russian\n",
      "Grid search score: 0.355\n",
      "Validation score: 0.31\n",
      "{'warm_start': True, 'solver': 'sag', 'penalty': 'l2', 'max_iter': 5000, 'l1_ratio': 0.5, 'fit_intercept': True, 'C': 0.1}\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "score = []\n",
    "best_models = {}\n",
    "random.seed(42)\n",
    "for language in data.language.unique():\n",
    "    X = data[data.language == language].sample(frac=1, random_state=1).copy()\n",
    "    y = X['is_root'].copy()\n",
    "\n",
    "    groups = X['sentence']\n",
    "\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    train_idx, val_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "    X_train, X_val= X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    cv = StratifiedGroupKFold(n_splits=5)\n",
    "    groups = X_train['sentence']\n",
    "    \n",
    "    # For Japanese we use an extra trees classifier because logistic regression works poorly\n",
    "    model = BalancedRandomForestClassifier(\n",
    "    n_jobs=7,\n",
    "    random_state=2,\n",
    "    sampling_strategy='auto'   # balances classes by undersampling the majority\n",
    "    )\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 5],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    }\n",
    "    feature_cols = [\n",
    "    'sentence', 'degree', 'closeness', 'harmonic', 'betweeness', 'load', 'pagerank',\n",
    "    'eigenvector', 'katz', 'information', 'current_flow_betweeness',\n",
    "    'percolation', 'second_order', 'laplacian'\n",
    "    ]\n",
    "\n",
    "    grid_search = RandomizedSearchCV(n_iter=300,\n",
    "    estimator=model,\n",
    "    param_distributions=param_grid,\n",
    "    cv=cv,\n",
    "    scoring=root_prediction_score,\n",
    "    n_jobs=7,\n",
    "    # We want to get random combination of params for each different language\n",
    "    random_state=random.randint(1, 1000)\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train[feature_cols], y_train, groups=groups)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    current_score = root_prediction_score(grid_search.best_estimator_, X_val[feature_cols], y_val)\n",
    "    score.append(current_score)\n",
    "    print(language)\n",
    "    print('Grid search score: ' + str(grid_search.best_score_))\n",
    "    print('Validation score: ' + str(current_score))\n",
    "    print(best_params)\n",
    "    print('---------------------------------------')\n",
    "    \n",
    "\n",
    "    # Fit a new model on the full training data (including any previously held-out validation)\n",
    "    if language == 'Japanese':\n",
    "        final_model = ExtraTreesClassifier(**best_params, random_state=2, n_jobs=7)\n",
    "    else:\n",
    "        final_model = LogisticRegression(**best_params, random_state=2, n_jobs=7)\n",
    "    final_model.fit(X[feature_cols], y)\n",
    "    best_models[language] = final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf503aed",
   "metadata": {},
   "source": [
    "Printing the average score for the whole validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8a3221e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2957142857142857"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics.mean(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python ML-project(myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
