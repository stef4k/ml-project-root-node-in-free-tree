{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a55429ae",
   "metadata": {},
   "source": [
    "# Linear Approach\n",
    "Starting by importing the right libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c508369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit, StratifiedGroupKFold\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import statistics\n",
    "import random\n",
    "import warnings\n",
    "from sklearn.exceptions import FitFailedWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FitFailedWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82005bf0",
   "metadata": {},
   "source": [
    "Next we read the train data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f0da078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>sentence</th>\n",
       "      <th>vertex</th>\n",
       "      <th>n</th>\n",
       "      <th>degree</th>\n",
       "      <th>closeness</th>\n",
       "      <th>harmonic</th>\n",
       "      <th>betweeness</th>\n",
       "      <th>load</th>\n",
       "      <th>pagerank</th>\n",
       "      <th>eigenvector</th>\n",
       "      <th>katz</th>\n",
       "      <th>information</th>\n",
       "      <th>current_flow_betweeness</th>\n",
       "      <th>percolation</th>\n",
       "      <th>second_order</th>\n",
       "      <th>laplacian</th>\n",
       "      <th>is_root</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>0.133038</td>\n",
       "      <td>-0.558160</td>\n",
       "      <td>-0.308023</td>\n",
       "      <td>-0.676962</td>\n",
       "      <td>-0.676962</td>\n",
       "      <td>0.409538</td>\n",
       "      <td>-0.327923</td>\n",
       "      <td>0.074694</td>\n",
       "      <td>-0.558160</td>\n",
       "      <td>-0.676962</td>\n",
       "      <td>-0.676962</td>\n",
       "      <td>0.489782</td>\n",
       "      <td>-0.031273</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>-1.396899</td>\n",
       "      <td>-1.111953</td>\n",
       "      <td>-1.442852</td>\n",
       "      <td>-1.126101</td>\n",
       "      <td>-1.126101</td>\n",
       "      <td>-1.313638</td>\n",
       "      <td>-1.131636</td>\n",
       "      <td>-1.460142</td>\n",
       "      <td>-1.111953</td>\n",
       "      <td>-1.126101</td>\n",
       "      <td>-1.126101</td>\n",
       "      <td>1.149235</td>\n",
       "      <td>-1.469827</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>1.662975</td>\n",
       "      <td>0.111311</td>\n",
       "      <td>0.741550</td>\n",
       "      <td>0.135764</td>\n",
       "      <td>0.135764</td>\n",
       "      <td>1.885752</td>\n",
       "      <td>0.745846</td>\n",
       "      <td>1.519954</td>\n",
       "      <td>0.111311</td>\n",
       "      <td>0.135764</td>\n",
       "      <td>0.135764</td>\n",
       "      <td>-0.198147</td>\n",
       "      <td>1.407281</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>-1.396899</td>\n",
       "      <td>-0.618060</td>\n",
       "      <td>-0.907167</td>\n",
       "      <td>-1.126101</td>\n",
       "      <td>-1.126101</td>\n",
       "      <td>-1.449223</td>\n",
       "      <td>-0.639537</td>\n",
       "      <td>-1.315616</td>\n",
       "      <td>-0.618060</td>\n",
       "      <td>-1.126101</td>\n",
       "      <td>-1.126101</td>\n",
       "      <td>0.556481</td>\n",
       "      <td>-1.110188</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>0.133038</td>\n",
       "      <td>0.812630</td>\n",
       "      <td>0.880961</td>\n",
       "      <td>0.413803</td>\n",
       "      <td>0.413803</td>\n",
       "      <td>-0.074579</td>\n",
       "      <td>1.113070</td>\n",
       "      <td>0.390094</td>\n",
       "      <td>0.812630</td>\n",
       "      <td>0.413803</td>\n",
       "      <td>0.413803</td>\n",
       "      <td>-0.837635</td>\n",
       "      <td>0.688004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   language  sentence  vertex   n    degree  closeness  harmonic  betweeness  \\\n",
       "0  Japanese         2       6  23  0.133038  -0.558160 -0.308023   -0.676962   \n",
       "1  Japanese         2       4  23 -1.396899  -1.111953 -1.442852   -1.126101   \n",
       "2  Japanese         2       2  23  1.662975   0.111311  0.741550    0.135764   \n",
       "3  Japanese         2      23  23 -1.396899  -0.618060 -0.907167   -1.126101   \n",
       "4  Japanese         2      20  23  0.133038   0.812630  0.880961    0.413803   \n",
       "\n",
       "       load  pagerank  eigenvector      katz  information  \\\n",
       "0 -0.676962  0.409538    -0.327923  0.074694    -0.558160   \n",
       "1 -1.126101 -1.313638    -1.131636 -1.460142    -1.111953   \n",
       "2  0.135764  1.885752     0.745846  1.519954     0.111311   \n",
       "3 -1.126101 -1.449223    -0.639537 -1.315616    -0.618060   \n",
       "4  0.413803 -0.074579     1.113070  0.390094     0.812630   \n",
       "\n",
       "   current_flow_betweeness  percolation  second_order  laplacian  is_root  \n",
       "0                -0.676962    -0.676962      0.489782  -0.031273        0  \n",
       "1                -1.126101    -1.126101      1.149235  -1.469827        0  \n",
       "2                 0.135764     0.135764     -0.198147   1.407281        0  \n",
       "3                -1.126101    -1.126101      0.556481  -1.110188        0  \n",
       "4                 0.413803     0.413803     -0.837635   0.688004        0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/normalized_expanded_train.csv')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e921a9ca",
   "metadata": {},
   "source": [
    "Now we will focus only in one language, opting to go for `Polish`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83483938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>sentence</th>\n",
       "      <th>vertex</th>\n",
       "      <th>n</th>\n",
       "      <th>degree</th>\n",
       "      <th>closeness</th>\n",
       "      <th>harmonic</th>\n",
       "      <th>betweeness</th>\n",
       "      <th>load</th>\n",
       "      <th>pagerank</th>\n",
       "      <th>eigenvector</th>\n",
       "      <th>katz</th>\n",
       "      <th>information</th>\n",
       "      <th>current_flow_betweeness</th>\n",
       "      <th>percolation</th>\n",
       "      <th>second_order</th>\n",
       "      <th>laplacian</th>\n",
       "      <th>is_root</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>126122</th>\n",
       "      <td>Polish</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>-0.983415</td>\n",
       "      <td>-1.112357</td>\n",
       "      <td>-1.140267</td>\n",
       "      <td>-0.862200</td>\n",
       "      <td>-0.862200</td>\n",
       "      <td>-0.946343</td>\n",
       "      <td>-0.721122</td>\n",
       "      <td>-0.987128</td>\n",
       "      <td>-1.112357</td>\n",
       "      <td>-0.862200</td>\n",
       "      <td>-0.862200</td>\n",
       "      <td>1.154875</td>\n",
       "      <td>-0.898371</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130297</th>\n",
       "      <td>Polish</td>\n",
       "      <td>567</td>\n",
       "      <td>25</td>\n",
       "      <td>35</td>\n",
       "      <td>-0.931149</td>\n",
       "      <td>-1.425134</td>\n",
       "      <td>-1.327955</td>\n",
       "      <td>-0.808350</td>\n",
       "      <td>-0.808350</td>\n",
       "      <td>-0.898689</td>\n",
       "      <td>-0.937838</td>\n",
       "      <td>-0.938361</td>\n",
       "      <td>-1.425134</td>\n",
       "      <td>-0.808350</td>\n",
       "      <td>-0.808350</td>\n",
       "      <td>1.544666</td>\n",
       "      <td>-0.839108</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128519</th>\n",
       "      <td>Polish</td>\n",
       "      <td>357</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.816497</td>\n",
       "      <td>-0.465846</td>\n",
       "      <td>-0.528847</td>\n",
       "      <td>-0.836117</td>\n",
       "      <td>-0.836117</td>\n",
       "      <td>-0.841449</td>\n",
       "      <td>-0.189595</td>\n",
       "      <td>-0.724639</td>\n",
       "      <td>-0.465846</td>\n",
       "      <td>-0.836117</td>\n",
       "      <td>-0.836117</td>\n",
       "      <td>0.404574</td>\n",
       "      <td>-0.525001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132310</th>\n",
       "      <td>Polish</td>\n",
       "      <td>825</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>2.083023</td>\n",
       "      <td>0.453251</td>\n",
       "      <td>1.350470</td>\n",
       "      <td>1.156678</td>\n",
       "      <td>1.156678</td>\n",
       "      <td>2.162341</td>\n",
       "      <td>0.520670</td>\n",
       "      <td>1.968188</td>\n",
       "      <td>0.453251</td>\n",
       "      <td>1.156678</td>\n",
       "      <td>1.156678</td>\n",
       "      <td>-0.514653</td>\n",
       "      <td>1.923926</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129666</th>\n",
       "      <td>Polish</td>\n",
       "      <td>490</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>0.068199</td>\n",
       "      <td>0.728343</td>\n",
       "      <td>0.712585</td>\n",
       "      <td>0.420129</td>\n",
       "      <td>0.420129</td>\n",
       "      <td>-0.047339</td>\n",
       "      <td>0.516672</td>\n",
       "      <td>0.236797</td>\n",
       "      <td>0.728343</td>\n",
       "      <td>0.420129</td>\n",
       "      <td>0.420129</td>\n",
       "      <td>-0.753390</td>\n",
       "      <td>0.259074</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       language  sentence  vertex   n    degree  closeness  harmonic  \\\n",
       "126122   Polish        46       2  23 -0.983415  -1.112357 -1.140267   \n",
       "130297   Polish       567      25  35 -0.931149  -1.425134 -1.327955   \n",
       "128519   Polish       357       9  10 -0.816497  -0.465846 -0.528847   \n",
       "132310   Polish       825      10  15  2.083023   0.453251  1.350470   \n",
       "129666   Polish       490      26  27  0.068199   0.728343  0.712585   \n",
       "\n",
       "        betweeness      load  pagerank  eigenvector      katz  information  \\\n",
       "126122   -0.862200 -0.862200 -0.946343    -0.721122 -0.987128    -1.112357   \n",
       "130297   -0.808350 -0.808350 -0.898689    -0.937838 -0.938361    -1.425134   \n",
       "128519   -0.836117 -0.836117 -0.841449    -0.189595 -0.724639    -0.465846   \n",
       "132310    1.156678  1.156678  2.162341     0.520670  1.968188     0.453251   \n",
       "129666    0.420129  0.420129 -0.047339     0.516672  0.236797     0.728343   \n",
       "\n",
       "        current_flow_betweeness  percolation  second_order  laplacian  is_root  \n",
       "126122                -0.862200    -0.862200      1.154875  -0.898371        0  \n",
       "130297                -0.808350    -0.808350      1.544666  -0.839108        0  \n",
       "128519                -0.836117    -0.836117      0.404574  -0.525001        0  \n",
       "132310                 1.156678     1.156678     -0.514653   1.923926        0  \n",
       "129666                 0.420129     0.420129     -0.753390   0.259074        0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polish_data = data[data.language == 'Polish'].sample(frac=1, random_state=1).copy()\n",
    "polish_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f4a227",
   "metadata": {},
   "source": [
    "Now we can split the data in train and test in a way to keep the same sentences (sentence level and node level of sentence) in the same group. Each sentence and its nodes will only be found in train or validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7693d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = polish_data.copy()#.drop(['is_root','language','n'], axis=1).copy()\n",
    "y = polish_data['is_root'].copy()\n",
    "\n",
    "groups = polish_data['sentence']\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "X_train, X_val= X.iloc[train_idx], X.iloc[val_idx]\n",
    "y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c84185",
   "metadata": {},
   "source": [
    "Now we also define the cross validation strategy. We also split the training data in 5 different folds ensuring that data of of the same sentence can not be found in different folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c45bd94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedGroupKFold(n_splits=5)\n",
    "groups = X_train['sentence']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaec98e",
   "metadata": {},
   "source": [
    "Next we can define the logistic regression model parameters that we want to try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af856faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=2, n_jobs=7)\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
    "    'l1_ratio': [0, 0.5, 1],\n",
    "    'fit_intercept':[True,False],\n",
    "    'warm_start': [True,False],\n",
    "    'max_iter': [1000, 2500, 5000]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc584422",
   "metadata": {},
   "source": [
    "We use a custom scoring method that picks one root per sentence. For each sentence, it chooses the node with the highest chance of being the root (class 1). The final score is just the percentage of sentences where we picked the correct root.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bbde6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_prediction_score(estimator, X, y_true):\n",
    "    \"\"\"\n",
    "    Scoring function that extracts sentence IDs from X and computes\n",
    "    root prediction accuracy per sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    sentence_ids = X['sentence'].values\n",
    "    X_features = X.copy()\n",
    "    # Predict probabilities\n",
    "    probs = estimator.predict_proba(X_features)[:, 1]\n",
    "    # Build DataFrame for groupby\n",
    "    df_pred = pd.DataFrame({\n",
    "        'sentence': sentence_ids,\n",
    "        'is_root': y_true,\n",
    "        'root_prob': probs\n",
    "    })\n",
    "\n",
    "    predicted_roots = df_pred.loc[df_pred.groupby('sentence')['root_prob'].idxmax()]\n",
    "    accuracy = float((predicted_roots['is_root'] == 1).mean())\n",
    "    return accuracy\n",
    "\n",
    "root_scorer = make_scorer(root_prediction_score, greater_is_better=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b790f57",
   "metadata": {},
   "source": [
    "Now we can do the gridsearch, combining defined parameters, the custom scoring function, the cross validation strategy (that ensures no data leakage):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aae5bce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'warm_start': False, 'solver': 'newton-cholesky', 'penalty': 'l2', 'max_iter': 5000, 'l1_ratio': 1, 'fit_intercept': True, 'C': 0.1}\n",
      "Best score: 0.27\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_cols = [\n",
    "    'sentence', 'degree', 'closeness', 'harmonic', 'betweeness', 'load', 'pagerank',\n",
    "    'eigenvector', 'katz', 'information', 'current_flow_betweeness',\n",
    "    'percolation', 'second_order', 'laplacian'\n",
    "]\n",
    "\n",
    "grid_search = RandomizedSearchCV(n_iter=200,\n",
    "    estimator=model,\n",
    "    param_distributions=param_grid,\n",
    "    cv=cv,\n",
    "    scoring=root_prediction_score,\n",
    "    n_jobs=7,\n",
    "    random_state=2\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train[feature_cols], y_train, groups=groups)\n",
    "\n",
    "print(\"Best params:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104a3cd3",
   "metadata": {},
   "source": [
    "We will use the best estimator found to see the performance in the validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df451647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_estimator = grid_search.best_estimator_\n",
    "root_prediction_score(best_estimator, X_val[feature_cols], y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b346f0ab",
   "metadata": {},
   "source": [
    "### Extending all languages\n",
    "Next we create a script that follows the previous steps for each of the 21 languages. As a result we will have one logistic regression model for each language. To predict the test data, we will use the particular logistic regression depending on the language of the row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce52cf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Japanese\n",
      "Grid search score: 0.12000000000000002\n",
      "Validation score: 0.11\n",
      "{'n_estimators': 400, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 10, 'criterion': 'gini', 'bootstrap': True}\n",
      "---------------------------------------\n",
      "Finnish\n",
      "Grid search score: 0.34750000000000003\n",
      "Validation score: 0.36\n",
      "{'warm_start': True, 'solver': 'lbfgs', 'penalty': 'l2', 'max_iter': 5000, 'l1_ratio': 0.5, 'fit_intercept': True, 'C': 100}\n",
      "---------------------------------------\n",
      "Galician\n",
      "Grid search score: 0.28500000000000003\n",
      "Validation score: 0.3\n",
      "{'warm_start': True, 'solver': 'lbfgs', 'penalty': None, 'max_iter': 5000, 'l1_ratio': 0.5, 'fit_intercept': False, 'C': 100}\n",
      "---------------------------------------\n",
      "English\n",
      "Grid search score: 0.29250000000000004\n",
      "Validation score: 0.28\n",
      "{'warm_start': True, 'solver': 'lbfgs', 'penalty': None, 'max_iter': 5000, 'l1_ratio': 1, 'fit_intercept': True, 'C': 100}\n",
      "---------------------------------------\n",
      "Hindi\n",
      "Grid search score: 0.2125\n",
      "Validation score: 0.27\n",
      "{'warm_start': True, 'solver': 'lbfgs', 'penalty': 'l2', 'max_iter': 2500, 'l1_ratio': 0.5, 'fit_intercept': True, 'C': 1}\n",
      "---------------------------------------\n",
      "French\n",
      "Grid search score: 0.26749999999999996\n",
      "Validation score: 0.29\n",
      "{'warm_start': True, 'solver': 'lbfgs', 'penalty': 'l2', 'max_iter': 5000, 'l1_ratio': 1, 'fit_intercept': True, 'C': 1}\n",
      "---------------------------------------\n",
      "Italian\n",
      "Grid search score: 0.27749999999999997\n",
      "Validation score: 0.2\n",
      "{'warm_start': True, 'solver': 'lbfgs', 'penalty': None, 'max_iter': 2500, 'l1_ratio': 0, 'fit_intercept': False, 'C': 100}\n",
      "---------------------------------------\n",
      "Indonesian\n",
      "Grid search score: 0.3075\n",
      "Validation score: 0.28\n",
      "{'warm_start': True, 'solver': 'lbfgs', 'penalty': None, 'max_iter': 1000, 'l1_ratio': 1, 'fit_intercept': False, 'C': 0.01}\n",
      "---------------------------------------\n",
      "Swedish\n",
      "Grid search score: 0.3125\n",
      "Validation score: 0.34\n",
      "{'warm_start': True, 'solver': 'sag', 'penalty': None, 'max_iter': 2500, 'l1_ratio': 0.5, 'fit_intercept': False, 'C': 1}\n",
      "---------------------------------------\n",
      "Spanish\n",
      "Grid search score: 0.315\n",
      "Validation score: 0.32\n",
      "{'warm_start': True, 'solver': 'sag', 'penalty': 'l2', 'max_iter': 5000, 'l1_ratio': 0.5, 'fit_intercept': False, 'C': 10}\n",
      "---------------------------------------\n",
      "Icelandic\n",
      "Grid search score: 0.3325\n",
      "Validation score: 0.33\n",
      "{'warm_start': False, 'solver': 'sag', 'penalty': 'l2', 'max_iter': 5000, 'l1_ratio': 1, 'fit_intercept': False, 'C': 1}\n",
      "---------------------------------------\n",
      "German\n",
      "Grid search score: 0.27499999999999997\n",
      "Validation score: 0.31\n",
      "{'warm_start': True, 'solver': 'lbfgs', 'penalty': None, 'max_iter': 1000, 'l1_ratio': 0.5, 'fit_intercept': False, 'C': 100}\n",
      "---------------------------------------\n",
      "Korean\n",
      "Grid search score: 0.3125\n",
      "Validation score: 0.34\n",
      "{'warm_start': True, 'solver': 'sag', 'penalty': 'l2', 'max_iter': 5000, 'l1_ratio': 0, 'fit_intercept': False, 'C': 100}\n",
      "---------------------------------------\n",
      "Polish\n",
      "Grid search score: 0.27\n",
      "Validation score: 0.32\n",
      "{'warm_start': True, 'solver': 'saga', 'penalty': 'l1', 'max_iter': 1000, 'l1_ratio': 1, 'fit_intercept': False, 'C': 10}\n",
      "---------------------------------------\n",
      "Thai\n",
      "Grid search score: 0.24\n",
      "Validation score: 0.3\n",
      "{'warm_start': True, 'solver': 'saga', 'penalty': 'l1', 'max_iter': 2500, 'l1_ratio': 0.5, 'fit_intercept': True, 'C': 0.01}\n",
      "---------------------------------------\n",
      "Turkish\n",
      "Grid search score: 0.3\n",
      "Validation score: 0.36\n",
      "{'warm_start': False, 'solver': 'saga', 'penalty': 'l2', 'max_iter': 1000, 'l1_ratio': 0.5, 'fit_intercept': True, 'C': 10}\n",
      "---------------------------------------\n",
      "Czech\n",
      "Grid search score: 0.27\n",
      "Validation score: 0.32\n",
      "{'warm_start': False, 'solver': 'saga', 'penalty': None, 'max_iter': 1000, 'l1_ratio': 0, 'fit_intercept': True, 'C': 0.01}\n",
      "---------------------------------------\n",
      "Chinese\n",
      "Grid search score: 0.2975\n",
      "Validation score: 0.26\n",
      "{'warm_start': True, 'solver': 'lbfgs', 'penalty': None, 'max_iter': 2500, 'l1_ratio': 0.5, 'fit_intercept': True, 'C': 10}\n",
      "---------------------------------------\n",
      "Portuguese\n",
      "Grid search score: 0.32\n",
      "Validation score: 0.28\n",
      "{'warm_start': False, 'solver': 'lbfgs', 'penalty': None, 'max_iter': 1000, 'l1_ratio': 0, 'fit_intercept': False, 'C': 0.1}\n",
      "---------------------------------------\n",
      "Arabic\n",
      "Grid search score: 0.28\n",
      "Validation score: 0.33\n",
      "{'warm_start': True, 'solver': 'saga', 'penalty': 'l2', 'max_iter': 1000, 'l1_ratio': 1, 'fit_intercept': True, 'C': 1}\n",
      "---------------------------------------\n",
      "Russian\n",
      "Grid search score: 0.355\n",
      "Validation score: 0.31\n",
      "{'warm_start': True, 'solver': 'sag', 'penalty': 'l2', 'max_iter': 5000, 'l1_ratio': 0.5, 'fit_intercept': True, 'C': 0.1}\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "score = []\n",
    "best_models = {}\n",
    "random.seed(42)\n",
    "for language in data.language.unique():\n",
    "    X = data[data.language == language].sample(frac=1, random_state=1).copy()\n",
    "    y = X['is_root'].copy()\n",
    "\n",
    "    groups = X['sentence']\n",
    "\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    train_idx, val_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "    X_train, X_val= X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    cv = StratifiedGroupKFold(n_splits=5)\n",
    "    groups = X_train['sentence']\n",
    "    \n",
    "    # For Japanese we use an extra trees classifier because logistic regression works poorly\n",
    "    if language == 'Japanese':\n",
    "        model = ExtraTreesClassifier(random_state=2, n_jobs=7)\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300, 400, 500],\n",
    "            'max_depth': [None, 10, 20],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'min_samples_leaf': [1, 2],\n",
    "            'max_features': ['sqrt', 'log2'],\n",
    "            'bootstrap': [True, False],\n",
    "            'criterion': ['gini', 'entropy', 'log_loss']\n",
    "\n",
    "        }\n",
    "    else:\n",
    "        model = LogisticRegression(random_state=2, n_jobs=7)\n",
    "        param_grid = {\n",
    "            'penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "            'C': [0.01, 0.1, 1, 10, 100],\n",
    "            'solver': ['lbfgs', 'sag', 'saga'],\n",
    "            'l1_ratio': [0, 0.5, 1],\n",
    "            'fit_intercept':[True,False],\n",
    "            'warm_start': [True,False],\n",
    "            'max_iter': [1000, 2500, 5000]\n",
    "        }\n",
    "    feature_cols = [\n",
    "    'sentence', 'degree', 'closeness', 'harmonic', 'betweeness', 'load', 'pagerank',\n",
    "    'eigenvector', 'katz', 'information', 'current_flow_betweeness',\n",
    "    'percolation', 'second_order', 'laplacian'\n",
    "    ]\n",
    "\n",
    "    grid_search = RandomizedSearchCV(n_iter=300,\n",
    "    estimator=model,\n",
    "    param_distributions=param_grid,\n",
    "    cv=cv,\n",
    "    scoring=root_prediction_score,\n",
    "    n_jobs=7,\n",
    "    # We want to get random combination of params for each different language\n",
    "    random_state=random.randint(1, 1000)\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train[feature_cols], y_train, groups=groups)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    current_score = root_prediction_score(grid_search.best_estimator_, X_val[feature_cols], y_val)\n",
    "    score.append(current_score)\n",
    "    print(language)\n",
    "    print('Grid search score: ' + str(grid_search.best_score_))\n",
    "    print('Validation score: ' + str(current_score))\n",
    "    print(best_params)\n",
    "    print('---------------------------------------')\n",
    "    \n",
    "\n",
    "    # Fit a new model on the full training data (including any previously held-out validation)\n",
    "    if language == 'Japanese':\n",
    "        final_model = ExtraTreesClassifier(**best_params, random_state=2, n_jobs=7)\n",
    "    else:\n",
    "        final_model = LogisticRegression(**best_params, random_state=2, n_jobs=7)\n",
    "    final_model.fit(X[feature_cols], y)\n",
    "    best_models[language] = final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf503aed",
   "metadata": {},
   "source": [
    "Printing the average score for the whole validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8a3221e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2957142857142857"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics.mean(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca5cede",
   "metadata": {},
   "source": [
    "## Test data\n",
    "Now let's use the best estimators found to predict the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d07fc824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>language</th>\n",
       "      <th>sentence</th>\n",
       "      <th>vertex</th>\n",
       "      <th>n</th>\n",
       "      <th>degree</th>\n",
       "      <th>closeness</th>\n",
       "      <th>harmonic</th>\n",
       "      <th>betweeness</th>\n",
       "      <th>load</th>\n",
       "      <th>pagerank</th>\n",
       "      <th>eigenvector</th>\n",
       "      <th>katz</th>\n",
       "      <th>information</th>\n",
       "      <th>current_flow_betweeness</th>\n",
       "      <th>percolation</th>\n",
       "      <th>second_order</th>\n",
       "      <th>laplacian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>43</td>\n",
       "      <td>0.043173</td>\n",
       "      <td>0.114668</td>\n",
       "      <td>0.270083</td>\n",
       "      <td>-0.710642</td>\n",
       "      <td>-0.710642</td>\n",
       "      <td>0.120017</td>\n",
       "      <td>-0.482073</td>\n",
       "      <td>0.031637</td>\n",
       "      <td>0.114668</td>\n",
       "      <td>-0.710642</td>\n",
       "      <td>-0.710642</td>\n",
       "      <td>-0.204072</td>\n",
       "      <td>-0.079853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>43</td>\n",
       "      <td>-0.885052</td>\n",
       "      <td>-0.428544</td>\n",
       "      <td>-0.934764</td>\n",
       "      <td>-0.951023</td>\n",
       "      <td>-0.951023</td>\n",
       "      <td>-0.801517</td>\n",
       "      <td>-0.482239</td>\n",
       "      <td>-1.040168</td>\n",
       "      <td>-0.428544</td>\n",
       "      <td>-0.951023</td>\n",
       "      <td>-0.951023</td>\n",
       "      <td>0.347011</td>\n",
       "      <td>-0.983454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>43</td>\n",
       "      <td>1.899625</td>\n",
       "      <td>0.766913</td>\n",
       "      <td>1.821878</td>\n",
       "      <td>0.743366</td>\n",
       "      <td>0.743366</td>\n",
       "      <td>1.764984</td>\n",
       "      <td>-0.481724</td>\n",
       "      <td>1.960983</td>\n",
       "      <td>0.766913</td>\n",
       "      <td>0.743366</td>\n",
       "      <td>0.743366</td>\n",
       "      <td>-0.799779</td>\n",
       "      <td>1.727348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>43</td>\n",
       "      <td>0.043173</td>\n",
       "      <td>-1.078191</td>\n",
       "      <td>-1.090006</td>\n",
       "      <td>-0.710642</td>\n",
       "      <td>-0.710642</td>\n",
       "      <td>0.299057</td>\n",
       "      <td>-0.482315</td>\n",
       "      <td>-0.174878</td>\n",
       "      <td>-1.078191</td>\n",
       "      <td>-0.710642</td>\n",
       "      <td>-0.710642</td>\n",
       "      <td>1.103667</td>\n",
       "      <td>-0.441293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>43</td>\n",
       "      <td>-0.885052</td>\n",
       "      <td>-1.405190</td>\n",
       "      <td>-1.871333</td>\n",
       "      <td>-0.951023</td>\n",
       "      <td>-0.951023</td>\n",
       "      <td>-0.725446</td>\n",
       "      <td>-0.482330</td>\n",
       "      <td>-1.060820</td>\n",
       "      <td>-1.405190</td>\n",
       "      <td>-0.951023</td>\n",
       "      <td>-0.951023</td>\n",
       "      <td>1.540413</td>\n",
       "      <td>-0.983454</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  language  sentence  vertex   n    degree  closeness  harmonic  \\\n",
       "0   1  Japanese         1      38  43  0.043173   0.114668  0.270083   \n",
       "1   1  Japanese         1      33  43 -0.885052  -0.428544 -0.934764   \n",
       "2   1  Japanese         1      10  43  1.899625   0.766913  1.821878   \n",
       "3   1  Japanese         1      24  43  0.043173  -1.078191 -1.090006   \n",
       "4   1  Japanese         1      16  43 -0.885052  -1.405190 -1.871333   \n",
       "\n",
       "   betweeness      load  pagerank  eigenvector      katz  information  \\\n",
       "0   -0.710642 -0.710642  0.120017    -0.482073  0.031637     0.114668   \n",
       "1   -0.951023 -0.951023 -0.801517    -0.482239 -1.040168    -0.428544   \n",
       "2    0.743366  0.743366  1.764984    -0.481724  1.960983     0.766913   \n",
       "3   -0.710642 -0.710642  0.299057    -0.482315 -0.174878    -1.078191   \n",
       "4   -0.951023 -0.951023 -0.725446    -0.482330 -1.060820    -1.405190   \n",
       "\n",
       "   current_flow_betweeness  percolation  second_order  laplacian  \n",
       "0                -0.710642    -0.710642     -0.204072  -0.079853  \n",
       "1                -0.951023    -0.951023      0.347011  -0.983454  \n",
       "2                 0.743366     0.743366     -0.799779   1.727348  \n",
       "3                -0.710642    -0.710642      1.103667  -0.441293  \n",
       "4                -0.951023    -0.951023      1.540413  -0.983454  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('../data/normalized_expanded_test.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6233c00",
   "metadata": {},
   "source": [
    "Next we create a script that for each sample of the test data  (unique id), uses the corresponding language model to make the predictions (binary classification node level predictions). Then the final prediction of the root node of the sentence instance id is the node with the highest probability to belong to the class 1: `root` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f12c23bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for test_sample_id in test.id.unique():\n",
    "    current_sample = test[test.id == test_sample_id].copy()\n",
    "    language = current_sample.language.iloc[0]\n",
    "    # Create a new column storing the probabilities\n",
    "    current_sample['probabilities'] = best_models[language].predict_proba(current_sample[feature_cols])[:, 1]\n",
    "    # Returning the vertex number of the row witht the highest probabilities\n",
    "    predicted_root = current_sample.loc[current_sample['probabilities'].idxmax(), 'vertex']\n",
    "    results.append({'id':test_sample_id, 'root':predicted_root})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b408b0",
   "metadata": {},
   "source": [
    "Now we can create a dataframe from the results and save it as csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b958d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results_df = pd.DataFrame(results)\n",
    "final_results_df.to_csv('../data/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3e8f77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python ML-project(myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
