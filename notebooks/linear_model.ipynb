{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a55429ae",
   "metadata": {},
   "source": [
    "# Linear Approach\n",
    "\n",
    "This approach focuses on using normalized centrality measures, a PCA and logistic regression for each of the languages.\n",
    "A pipeline is trained that uses PCA and a logistic regression for each of the 21 unique languages.\n",
    "\n",
    "Starting by importing the right libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c508369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit, StratifiedGroupKFold\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import make_scorer\n",
    "import statistics\n",
    "import random\n",
    "import warnings\n",
    "from sklearn.exceptions import FitFailedWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FitFailedWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82005bf0",
   "metadata": {},
   "source": [
    "Next we read the train data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f0da078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>sentence</th>\n",
       "      <th>vertex</th>\n",
       "      <th>n</th>\n",
       "      <th>degree</th>\n",
       "      <th>closeness</th>\n",
       "      <th>harmonic</th>\n",
       "      <th>betweeness</th>\n",
       "      <th>load</th>\n",
       "      <th>pagerank</th>\n",
       "      <th>eigenvector</th>\n",
       "      <th>katz</th>\n",
       "      <th>information</th>\n",
       "      <th>current_flow_betweeness</th>\n",
       "      <th>percolation</th>\n",
       "      <th>second_order</th>\n",
       "      <th>laplacian</th>\n",
       "      <th>is_root</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>0.133038</td>\n",
       "      <td>-0.558160</td>\n",
       "      <td>-0.308023</td>\n",
       "      <td>-0.676962</td>\n",
       "      <td>-0.676962</td>\n",
       "      <td>0.409538</td>\n",
       "      <td>-0.327923</td>\n",
       "      <td>0.074694</td>\n",
       "      <td>-0.558160</td>\n",
       "      <td>-0.676962</td>\n",
       "      <td>-0.676962</td>\n",
       "      <td>0.489782</td>\n",
       "      <td>-0.031273</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>-1.396899</td>\n",
       "      <td>-1.111953</td>\n",
       "      <td>-1.442852</td>\n",
       "      <td>-1.126101</td>\n",
       "      <td>-1.126101</td>\n",
       "      <td>-1.313638</td>\n",
       "      <td>-1.131636</td>\n",
       "      <td>-1.460142</td>\n",
       "      <td>-1.111953</td>\n",
       "      <td>-1.126101</td>\n",
       "      <td>-1.126101</td>\n",
       "      <td>1.149235</td>\n",
       "      <td>-1.469827</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>1.662975</td>\n",
       "      <td>0.111311</td>\n",
       "      <td>0.741550</td>\n",
       "      <td>0.135764</td>\n",
       "      <td>0.135764</td>\n",
       "      <td>1.885752</td>\n",
       "      <td>0.745846</td>\n",
       "      <td>1.519954</td>\n",
       "      <td>0.111311</td>\n",
       "      <td>0.135764</td>\n",
       "      <td>0.135764</td>\n",
       "      <td>-0.198147</td>\n",
       "      <td>1.407281</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>-1.396899</td>\n",
       "      <td>-0.618060</td>\n",
       "      <td>-0.907167</td>\n",
       "      <td>-1.126101</td>\n",
       "      <td>-1.126101</td>\n",
       "      <td>-1.449223</td>\n",
       "      <td>-0.639537</td>\n",
       "      <td>-1.315616</td>\n",
       "      <td>-0.618060</td>\n",
       "      <td>-1.126101</td>\n",
       "      <td>-1.126101</td>\n",
       "      <td>0.556481</td>\n",
       "      <td>-1.110188</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>0.133038</td>\n",
       "      <td>0.812630</td>\n",
       "      <td>0.880961</td>\n",
       "      <td>0.413803</td>\n",
       "      <td>0.413803</td>\n",
       "      <td>-0.074579</td>\n",
       "      <td>1.113070</td>\n",
       "      <td>0.390094</td>\n",
       "      <td>0.812630</td>\n",
       "      <td>0.413803</td>\n",
       "      <td>0.413803</td>\n",
       "      <td>-0.837635</td>\n",
       "      <td>0.688004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   language  sentence  vertex   n    degree  closeness  harmonic  betweeness  \\\n",
       "0  Japanese         2       6  23  0.133038  -0.558160 -0.308023   -0.676962   \n",
       "1  Japanese         2       4  23 -1.396899  -1.111953 -1.442852   -1.126101   \n",
       "2  Japanese         2       2  23  1.662975   0.111311  0.741550    0.135764   \n",
       "3  Japanese         2      23  23 -1.396899  -0.618060 -0.907167   -1.126101   \n",
       "4  Japanese         2      20  23  0.133038   0.812630  0.880961    0.413803   \n",
       "\n",
       "       load  pagerank  eigenvector      katz  information  \\\n",
       "0 -0.676962  0.409538    -0.327923  0.074694    -0.558160   \n",
       "1 -1.126101 -1.313638    -1.131636 -1.460142    -1.111953   \n",
       "2  0.135764  1.885752     0.745846  1.519954     0.111311   \n",
       "3 -1.126101 -1.449223    -0.639537 -1.315616    -0.618060   \n",
       "4  0.413803 -0.074579     1.113070  0.390094     0.812630   \n",
       "\n",
       "   current_flow_betweeness  percolation  second_order  laplacian  is_root  \n",
       "0                -0.676962    -0.676962      0.489782  -0.031273        0  \n",
       "1                -1.126101    -1.126101      1.149235  -1.469827        0  \n",
       "2                 0.135764     0.135764     -0.198147   1.407281        0  \n",
       "3                -1.126101    -1.126101      0.556481  -1.110188        0  \n",
       "4                 0.413803     0.413803     -0.837635   0.688004        0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/normalized_expanded_train.csv')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e921a9ca",
   "metadata": {},
   "source": [
    "Now we will focus only in one language, opting to go for `Polish`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83483938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>sentence</th>\n",
       "      <th>vertex</th>\n",
       "      <th>n</th>\n",
       "      <th>degree</th>\n",
       "      <th>closeness</th>\n",
       "      <th>harmonic</th>\n",
       "      <th>betweeness</th>\n",
       "      <th>load</th>\n",
       "      <th>pagerank</th>\n",
       "      <th>eigenvector</th>\n",
       "      <th>katz</th>\n",
       "      <th>information</th>\n",
       "      <th>current_flow_betweeness</th>\n",
       "      <th>percolation</th>\n",
       "      <th>second_order</th>\n",
       "      <th>laplacian</th>\n",
       "      <th>is_root</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>126122</th>\n",
       "      <td>Polish</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>-0.983415</td>\n",
       "      <td>-1.112357</td>\n",
       "      <td>-1.140267</td>\n",
       "      <td>-0.862200</td>\n",
       "      <td>-0.862200</td>\n",
       "      <td>-0.946343</td>\n",
       "      <td>-0.721122</td>\n",
       "      <td>-0.987128</td>\n",
       "      <td>-1.112357</td>\n",
       "      <td>-0.862200</td>\n",
       "      <td>-0.862200</td>\n",
       "      <td>1.154875</td>\n",
       "      <td>-0.898371</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130297</th>\n",
       "      <td>Polish</td>\n",
       "      <td>567</td>\n",
       "      <td>25</td>\n",
       "      <td>35</td>\n",
       "      <td>-0.931149</td>\n",
       "      <td>-1.425134</td>\n",
       "      <td>-1.327955</td>\n",
       "      <td>-0.808350</td>\n",
       "      <td>-0.808350</td>\n",
       "      <td>-0.898689</td>\n",
       "      <td>-0.937838</td>\n",
       "      <td>-0.938361</td>\n",
       "      <td>-1.425134</td>\n",
       "      <td>-0.808350</td>\n",
       "      <td>-0.808350</td>\n",
       "      <td>1.544666</td>\n",
       "      <td>-0.839108</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128519</th>\n",
       "      <td>Polish</td>\n",
       "      <td>357</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.816497</td>\n",
       "      <td>-0.465846</td>\n",
       "      <td>-0.528847</td>\n",
       "      <td>-0.836117</td>\n",
       "      <td>-0.836117</td>\n",
       "      <td>-0.841449</td>\n",
       "      <td>-0.189595</td>\n",
       "      <td>-0.724639</td>\n",
       "      <td>-0.465846</td>\n",
       "      <td>-0.836117</td>\n",
       "      <td>-0.836117</td>\n",
       "      <td>0.404574</td>\n",
       "      <td>-0.525001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132310</th>\n",
       "      <td>Polish</td>\n",
       "      <td>825</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>2.083023</td>\n",
       "      <td>0.453251</td>\n",
       "      <td>1.350470</td>\n",
       "      <td>1.156678</td>\n",
       "      <td>1.156678</td>\n",
       "      <td>2.162341</td>\n",
       "      <td>0.520670</td>\n",
       "      <td>1.968188</td>\n",
       "      <td>0.453251</td>\n",
       "      <td>1.156678</td>\n",
       "      <td>1.156678</td>\n",
       "      <td>-0.514653</td>\n",
       "      <td>1.923926</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129666</th>\n",
       "      <td>Polish</td>\n",
       "      <td>490</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>0.068199</td>\n",
       "      <td>0.728343</td>\n",
       "      <td>0.712585</td>\n",
       "      <td>0.420129</td>\n",
       "      <td>0.420129</td>\n",
       "      <td>-0.047339</td>\n",
       "      <td>0.516672</td>\n",
       "      <td>0.236797</td>\n",
       "      <td>0.728343</td>\n",
       "      <td>0.420129</td>\n",
       "      <td>0.420129</td>\n",
       "      <td>-0.753390</td>\n",
       "      <td>0.259074</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       language  sentence  vertex   n    degree  closeness  harmonic  \\\n",
       "126122   Polish        46       2  23 -0.983415  -1.112357 -1.140267   \n",
       "130297   Polish       567      25  35 -0.931149  -1.425134 -1.327955   \n",
       "128519   Polish       357       9  10 -0.816497  -0.465846 -0.528847   \n",
       "132310   Polish       825      10  15  2.083023   0.453251  1.350470   \n",
       "129666   Polish       490      26  27  0.068199   0.728343  0.712585   \n",
       "\n",
       "        betweeness      load  pagerank  eigenvector      katz  information  \\\n",
       "126122   -0.862200 -0.862200 -0.946343    -0.721122 -0.987128    -1.112357   \n",
       "130297   -0.808350 -0.808350 -0.898689    -0.937838 -0.938361    -1.425134   \n",
       "128519   -0.836117 -0.836117 -0.841449    -0.189595 -0.724639    -0.465846   \n",
       "132310    1.156678  1.156678  2.162341     0.520670  1.968188     0.453251   \n",
       "129666    0.420129  0.420129 -0.047339     0.516672  0.236797     0.728343   \n",
       "\n",
       "        current_flow_betweeness  percolation  second_order  laplacian  is_root  \n",
       "126122                -0.862200    -0.862200      1.154875  -0.898371        0  \n",
       "130297                -0.808350    -0.808350      1.544666  -0.839108        0  \n",
       "128519                -0.836117    -0.836117      0.404574  -0.525001        0  \n",
       "132310                 1.156678     1.156678     -0.514653   1.923926        0  \n",
       "129666                 0.420129     0.420129     -0.753390   0.259074        0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polish_data = data[data.language == 'Polish'].sample(frac=1, random_state=1).copy()\n",
    "polish_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190efb91",
   "metadata": {},
   "source": [
    "Now we can use pca to reduce the centrality dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "141985d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=7)\n",
    "feature_cols = ['degree', 'closeness','harmonic', 'betweeness', 'load', 'pagerank', 'eigenvector', 'katz',\n",
    "       'information', 'current_flow_betweeness', 'percolation', 'second_order',\n",
    "       'laplacian']\n",
    "X_pca = pca.fit_transform(polish_data[feature_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec374cae",
   "metadata": {},
   "source": [
    "The explained variance reaches almost 100% by just using 7 principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd3948e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9999398498243797)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f4a227",
   "metadata": {},
   "source": [
    "Now we can split the data in train and test in a way to keep the same sentences (sentence level and node level of sentence) in the same group. Each sentence and its nodes will only be found in train or validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7693d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = polish_data.copy()#.drop(['is_root','language','n'], axis=1).copy()\n",
    "y = polish_data['is_root'].copy()\n",
    "\n",
    "groups = polish_data['sentence']\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "X_train, X_val= X.iloc[train_idx], X.iloc[val_idx]\n",
    "y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c84185",
   "metadata": {},
   "source": [
    "Now we also define the cross validation strategy. We also split the training data in 5 different folds ensuring that data of the same sentence can not be found in different folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c45bd94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedGroupKFold(n_splits=5)\n",
    "groups = X_train['sentence']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaec98e",
   "metadata": {},
   "source": [
    "Next we can define the pipeline of pca and logistic regression model parameters that we want to try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af856faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns\n",
    "feature_cols = ['sentence', 'degree', 'closeness','harmonic', 'betweeness', 'load', 'pagerank', 'eigenvector', 'katz',\n",
    "       'information', 'current_flow_betweeness', 'percolation', 'second_order',\n",
    "       'laplacian']\n",
    "\n",
    "pca_columns = X_train[feature_cols].columns.difference(['sentence'])\n",
    "passthrough_columns = ['sentence']\n",
    "\n",
    "# We want the sentence column to passthrough and not be transformed since it is used for the scoring\n",
    "preprocessor = ColumnTransformer([\n",
    "     ('pca_pipeline', Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=7))\n",
    "    ]), pca_columns),\n",
    "    ('passthrough', 'passthrough', passthrough_columns)\n",
    "])\n",
    "\n",
    "# Full pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=2, n_jobs=7))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "    'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
    "    'classifier__solver': ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
    "    'classifier__l1_ratio': [0, 0.5, 1],\n",
    "    'classifier__fit_intercept':[True,False],\n",
    "    'classifier__warm_start': [True,False],\n",
    "    'classifier__max_iter': [100, 250, 500, 1000, 2500, 5000]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc584422",
   "metadata": {},
   "source": [
    "We use a custom scoring method that picks one root per sentence. For each sentence, it chooses the node with the highest chance of being the root (class 1). The final score is just the percentage of sentences where we picked the correct root.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3bbde6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_prediction_score(estimator, X, y_true):\n",
    "    \"\"\"\n",
    "    Scoring function that extracts sentence IDs from X and computes\n",
    "    root prediction accuracy per sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    sentence_ids = X['sentence'].values\n",
    "    X_features = X.copy()\n",
    "    # Predict probabilities\n",
    "    probs = estimator.predict_proba(X_features)[:, 1]\n",
    "    # Build DataFrame for groupby\n",
    "    df_pred = pd.DataFrame({\n",
    "        'sentence': sentence_ids,\n",
    "        'is_root': y_true,\n",
    "        'root_prob': probs\n",
    "    })\n",
    "\n",
    "    predicted_roots = df_pred.loc[df_pred.groupby('sentence')['root_prob'].idxmax()]\n",
    "    accuracy = float((predicted_roots['is_root'] == 1).mean())\n",
    "    return accuracy\n",
    "\n",
    "root_scorer = make_scorer(root_prediction_score, greater_is_better=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b790f57",
   "metadata": {},
   "source": [
    "Now we can do the gridsearch, combining defined parameters, the custom scoring function, the cross validation strategy (that ensures no data leakage):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aae5bce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'classifier__warm_start': False, 'classifier__solver': 'saga', 'classifier__penalty': 'elasticnet', 'classifier__max_iter': 250, 'classifier__l1_ratio': 1, 'classifier__fit_intercept': True, 'classifier__C': 0.01}\n",
      "Best score: 0.2725\n"
     ]
    }
   ],
   "source": [
    "\n",
    "grid_search = RandomizedSearchCV(n_iter=500,\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_grid,\n",
    "    cv=cv,\n",
    "    scoring=root_prediction_score,\n",
    "    n_jobs=7,\n",
    "    random_state=2\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train[feature_cols], y_train, groups=groups)\n",
    "\n",
    "print(\"Best params:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104a3cd3",
   "metadata": {},
   "source": [
    "We will use the best estimator found to see the performance in the validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df451647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_estimator = grid_search.best_estimator_\n",
    "root_prediction_score(best_estimator, X_val[feature_cols], y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b346f0ab",
   "metadata": {},
   "source": [
    "### Extending all languages\n",
    "Next we create a script that follows the previous steps for each of the 21 languages. As a result we will have one logistic regression model for each language. To predict the test data, we will use the particular logistic regression depending on the language of the row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce52cf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Japanese\n",
      "Grid search score: 0.095\n",
      "Validation score: 0.09\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 5, 'classifier__warm_start': True, 'classifier__solver': 'newton-cg', 'classifier__penalty': None, 'classifier__max_iter': 500, 'classifier__l1_ratio': 1, 'classifier__fit_intercept': True, 'classifier__C': 0.1}\n",
      "---------------------------------------\n",
      "Finnish\n",
      "Grid search score: 0.35750000000000004\n",
      "Validation score: 0.38\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 8, 'classifier__warm_start': True, 'classifier__solver': 'newton-cg', 'classifier__penalty': 'l2', 'classifier__max_iter': 2000, 'classifier__l1_ratio': 1, 'classifier__fit_intercept': True, 'classifier__C': 10}\n",
      "---------------------------------------\n",
      "Galician\n",
      "Grid search score: 0.295\n",
      "Validation score: 0.32\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 9, 'classifier__warm_start': True, 'classifier__solver': 'newton-cholesky', 'classifier__penalty': None, 'classifier__max_iter': 500, 'classifier__l1_ratio': 1, 'classifier__fit_intercept': True, 'classifier__C': 10}\n",
      "---------------------------------------\n",
      "English\n",
      "Grid search score: 0.3\n",
      "Validation score: 0.28\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 9, 'classifier__warm_start': True, 'classifier__solver': 'liblinear', 'classifier__penalty': 'l1', 'classifier__max_iter': 2000, 'classifier__l1_ratio': 0, 'classifier__fit_intercept': True, 'classifier__C': 100}\n",
      "---------------------------------------\n",
      "Hindi\n",
      "Grid search score: 0.21749999999999997\n",
      "Validation score: 0.26\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 9, 'classifier__warm_start': True, 'classifier__solver': 'newton-cholesky', 'classifier__penalty': None, 'classifier__max_iter': 750, 'classifier__l1_ratio': 0, 'classifier__fit_intercept': True, 'classifier__C': 100}\n",
      "---------------------------------------\n",
      "French\n",
      "Grid search score: 0.2725000000000001\n",
      "Validation score: 0.29\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 9, 'classifier__warm_start': False, 'classifier__solver': 'lbfgs', 'classifier__penalty': 'l2', 'classifier__max_iter': 250, 'classifier__l1_ratio': 0, 'classifier__fit_intercept': True, 'classifier__C': 10}\n",
      "---------------------------------------\n",
      "Italian\n",
      "Grid search score: 0.27749999999999997\n",
      "Validation score: 0.26\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 9, 'classifier__warm_start': False, 'classifier__solver': 'newton-cg', 'classifier__penalty': 'l2', 'classifier__max_iter': 500, 'classifier__l1_ratio': 0.5, 'classifier__fit_intercept': True, 'classifier__C': 1}\n",
      "---------------------------------------\n",
      "Indonesian\n",
      "Grid search score: 0.3125\n",
      "Validation score: 0.3\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 9, 'classifier__warm_start': True, 'classifier__solver': 'newton-cholesky', 'classifier__penalty': None, 'classifier__max_iter': 1000, 'classifier__l1_ratio': 0, 'classifier__fit_intercept': True, 'classifier__C': 0.1}\n",
      "---------------------------------------\n",
      "Swedish\n",
      "Grid search score: 0.31500000000000006\n",
      "Validation score: 0.31\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 8, 'classifier__warm_start': False, 'classifier__solver': 'newton-cholesky', 'classifier__penalty': 'l2', 'classifier__max_iter': 100, 'classifier__l1_ratio': 1, 'classifier__fit_intercept': True, 'classifier__C': 1}\n",
      "---------------------------------------\n",
      "Spanish\n",
      "Grid search score: 0.32\n",
      "Validation score: 0.3\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 9, 'classifier__warm_start': False, 'classifier__solver': 'newton-cg', 'classifier__penalty': None, 'classifier__max_iter': 1000, 'classifier__l1_ratio': 0, 'classifier__fit_intercept': False, 'classifier__C': 1}\n",
      "---------------------------------------\n",
      "Icelandic\n",
      "Grid search score: 0.335\n",
      "Validation score: 0.33\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 8, 'classifier__warm_start': True, 'classifier__solver': 'liblinear', 'classifier__penalty': 'l2', 'classifier__max_iter': 50, 'classifier__l1_ratio': 1, 'classifier__fit_intercept': False, 'classifier__C': 1}\n",
      "---------------------------------------\n",
      "German\n",
      "Grid search score: 0.27749999999999997\n",
      "Validation score: 0.31\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 8, 'classifier__warm_start': True, 'classifier__solver': 'newton-cg', 'classifier__penalty': None, 'classifier__max_iter': 500, 'classifier__l1_ratio': 0.5, 'classifier__fit_intercept': False, 'classifier__C': 100}\n",
      "---------------------------------------\n",
      "Korean\n",
      "Grid search score: 0.3125\n",
      "Validation score: 0.33\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 5, 'classifier__warm_start': False, 'classifier__solver': 'liblinear', 'classifier__penalty': 'l2', 'classifier__max_iter': 500, 'classifier__l1_ratio': 0, 'classifier__fit_intercept': False, 'classifier__C': 1}\n",
      "---------------------------------------\n",
      "Polish\n",
      "Grid search score: 0.27499999999999997\n",
      "Validation score: 0.33\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 5, 'classifier__warm_start': False, 'classifier__solver': 'saga', 'classifier__penalty': 'l1', 'classifier__max_iter': 1000, 'classifier__l1_ratio': 0.5, 'classifier__fit_intercept': False, 'classifier__C': 0.01}\n",
      "---------------------------------------\n",
      "Thai\n",
      "Grid search score: 0.2475\n",
      "Validation score: 0.28\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 5, 'classifier__warm_start': True, 'classifier__solver': 'liblinear', 'classifier__penalty': 'l2', 'classifier__max_iter': 50, 'classifier__l1_ratio': 0, 'classifier__fit_intercept': True, 'classifier__C': 10}\n",
      "---------------------------------------\n",
      "Turkish\n",
      "Grid search score: 0.305\n",
      "Validation score: 0.34\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 5, 'classifier__warm_start': False, 'classifier__solver': 'newton-cg', 'classifier__penalty': 'l2', 'classifier__max_iter': 1000, 'classifier__l1_ratio': 0.5, 'classifier__fit_intercept': True, 'classifier__C': 1}\n",
      "---------------------------------------\n",
      "Czech\n",
      "Grid search score: 0.2725\n",
      "Validation score: 0.33\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 7, 'classifier__warm_start': True, 'classifier__solver': 'liblinear', 'classifier__penalty': 'l1', 'classifier__max_iter': 500, 'classifier__l1_ratio': 0, 'classifier__fit_intercept': True, 'classifier__C': 0.1}\n",
      "---------------------------------------\n",
      "Chinese\n",
      "Grid search score: 0.2975\n",
      "Validation score: 0.28\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 9, 'classifier__warm_start': False, 'classifier__solver': 'liblinear', 'classifier__penalty': 'l1', 'classifier__max_iter': 250, 'classifier__l1_ratio': 0.5, 'classifier__fit_intercept': True, 'classifier__C': 100}\n",
      "---------------------------------------\n",
      "Portuguese\n",
      "Grid search score: 0.3325\n",
      "Validation score: 0.31\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 9, 'classifier__warm_start': True, 'classifier__solver': 'liblinear', 'classifier__penalty': 'l1', 'classifier__max_iter': 500, 'classifier__l1_ratio': 0.5, 'classifier__fit_intercept': False, 'classifier__C': 100}\n",
      "---------------------------------------\n",
      "Arabic\n",
      "Grid search score: 0.2875\n",
      "Validation score: 0.32\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 5, 'classifier__warm_start': True, 'classifier__solver': 'newton-cholesky', 'classifier__penalty': None, 'classifier__max_iter': 50, 'classifier__l1_ratio': 0.5, 'classifier__fit_intercept': True, 'classifier__C': 10}\n",
      "---------------------------------------\n",
      "Russian\n",
      "Grid search score: 0.35250000000000004\n",
      "Validation score: 0.3\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 7, 'classifier__warm_start': False, 'classifier__solver': 'newton-cholesky', 'classifier__penalty': 'l2', 'classifier__max_iter': 2000, 'classifier__l1_ratio': 0, 'classifier__fit_intercept': True, 'classifier__C': 0.1}\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "score = []\n",
    "best_models = {}\n",
    "random.seed(42)\n",
    "for language in data.language.unique():\n",
    "    X = data[data.language == language].sample(frac=1, random_state=1).copy()\n",
    "    y = X['is_root'].copy()\n",
    "\n",
    "    groups = X['sentence']\n",
    "\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    train_idx, val_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "    X_train, X_val= X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    cv = StratifiedGroupKFold(n_splits=5)\n",
    "    groups = X_train['sentence']\n",
    "    \n",
    "    feature_cols = [\n",
    "    'sentence', 'degree', 'closeness', 'harmonic', 'betweeness', 'load', 'pagerank',\n",
    "    'eigenvector', 'katz', 'information', 'current_flow_betweeness',\n",
    "    'percolation', 'second_order', 'laplacian'\n",
    "    ]\n",
    "    pca_columns = X_train[feature_cols].columns.difference(['sentence'])\n",
    "    passthrough_columns = ['sentence']\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('pca_pipeline', Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('pca', PCA())\n",
    "        ]), pca_columns),\n",
    "        ('passthrough', 'passthrough', passthrough_columns)\n",
    "    ])\n",
    "\n",
    "    # Final pipeline of pca and classifier\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', LogisticRegression(random_state=2, n_jobs=7))\n",
    "    ])\n",
    "\n",
    "    param_grid = {\n",
    "        'preprocessor__pca_pipeline__pca__n_components': [5,6,7,8,9],\n",
    "        'classifier__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "        'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'classifier__solver': ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
    "        'classifier__l1_ratio': [0, 0.5, 1],\n",
    "        'classifier__fit_intercept':[True,False],\n",
    "        'classifier__warm_start': [True,False],\n",
    "        'classifier__max_iter': [50, 100, 250, 500, 750, 1000, 2000]\n",
    "    }\n",
    "\n",
    "    grid_search = RandomizedSearchCV(n_iter=400,\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_grid,\n",
    "    cv=cv,\n",
    "    scoring=root_prediction_score,\n",
    "    n_jobs=7,\n",
    "    # We want to get random combination of params for each different language\n",
    "    random_state=random.randint(1, 1000)\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train[feature_cols], y_train, groups=groups)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    current_score = root_prediction_score(grid_search.best_estimator_, X_val[feature_cols], y_val)\n",
    "    score.append(current_score)\n",
    "    print(language)\n",
    "    print('Grid search score: ' + str(grid_search.best_score_))\n",
    "    print('Validation score: ' + str(current_score))\n",
    "    print(best_params)\n",
    "    print('---------------------------------------')\n",
    "    \n",
    "    # Rebuild the whole pipeline using the best params\n",
    "    final_preprocessor = ColumnTransformer([\n",
    "        ('pca_pipeline', Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('pca', PCA(n_components=best_params['preprocessor__pca_pipeline__pca__n_components']))\n",
    "        ]), pca_columns),\n",
    "        ('passthrough', 'passthrough', passthrough_columns)\n",
    "    ])\n",
    "\n",
    "    # Handle optional params\n",
    "    classifier_params = {\n",
    "        key.split(\"classifier__\")[1]: value\n",
    "        for key, value in best_params.items()\n",
    "        if key.startswith(\"classifier__\")\n",
    "    }\n",
    "\n",
    "    final_model = Pipeline([\n",
    "        ('preprocessor', final_preprocessor),\n",
    "        ('classifier', LogisticRegression(\n",
    "            **classifier_params,\n",
    "            n_jobs=7,\n",
    "            random_state=2\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "\n",
    "    final_model.fit(X[feature_cols], y)\n",
    "    best_models[language] = final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf503aed",
   "metadata": {},
   "source": [
    "Printing the average score for the whole validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8a3221e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2976190476190476"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics.mean(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca5cede",
   "metadata": {},
   "source": [
    "## Test data\n",
    "Now let's use the best estimators found to predict the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d07fc824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>language</th>\n",
       "      <th>sentence</th>\n",
       "      <th>vertex</th>\n",
       "      <th>n</th>\n",
       "      <th>degree</th>\n",
       "      <th>closeness</th>\n",
       "      <th>harmonic</th>\n",
       "      <th>betweeness</th>\n",
       "      <th>load</th>\n",
       "      <th>pagerank</th>\n",
       "      <th>eigenvector</th>\n",
       "      <th>katz</th>\n",
       "      <th>information</th>\n",
       "      <th>current_flow_betweeness</th>\n",
       "      <th>percolation</th>\n",
       "      <th>second_order</th>\n",
       "      <th>laplacian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>43</td>\n",
       "      <td>0.043173</td>\n",
       "      <td>0.114668</td>\n",
       "      <td>0.270083</td>\n",
       "      <td>-0.710642</td>\n",
       "      <td>-0.710642</td>\n",
       "      <td>0.120017</td>\n",
       "      <td>-0.482073</td>\n",
       "      <td>0.031637</td>\n",
       "      <td>0.114668</td>\n",
       "      <td>-0.710642</td>\n",
       "      <td>-0.710642</td>\n",
       "      <td>-0.204072</td>\n",
       "      <td>-0.079853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>43</td>\n",
       "      <td>-0.885052</td>\n",
       "      <td>-0.428544</td>\n",
       "      <td>-0.934764</td>\n",
       "      <td>-0.951023</td>\n",
       "      <td>-0.951023</td>\n",
       "      <td>-0.801517</td>\n",
       "      <td>-0.482239</td>\n",
       "      <td>-1.040168</td>\n",
       "      <td>-0.428544</td>\n",
       "      <td>-0.951023</td>\n",
       "      <td>-0.951023</td>\n",
       "      <td>0.347011</td>\n",
       "      <td>-0.983454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>43</td>\n",
       "      <td>1.899625</td>\n",
       "      <td>0.766913</td>\n",
       "      <td>1.821878</td>\n",
       "      <td>0.743366</td>\n",
       "      <td>0.743366</td>\n",
       "      <td>1.764984</td>\n",
       "      <td>-0.481724</td>\n",
       "      <td>1.960983</td>\n",
       "      <td>0.766913</td>\n",
       "      <td>0.743366</td>\n",
       "      <td>0.743366</td>\n",
       "      <td>-0.799779</td>\n",
       "      <td>1.727348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>43</td>\n",
       "      <td>0.043173</td>\n",
       "      <td>-1.078191</td>\n",
       "      <td>-1.090006</td>\n",
       "      <td>-0.710642</td>\n",
       "      <td>-0.710642</td>\n",
       "      <td>0.299057</td>\n",
       "      <td>-0.482315</td>\n",
       "      <td>-0.174878</td>\n",
       "      <td>-1.078191</td>\n",
       "      <td>-0.710642</td>\n",
       "      <td>-0.710642</td>\n",
       "      <td>1.103667</td>\n",
       "      <td>-0.441293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>43</td>\n",
       "      <td>-0.885052</td>\n",
       "      <td>-1.405190</td>\n",
       "      <td>-1.871333</td>\n",
       "      <td>-0.951023</td>\n",
       "      <td>-0.951023</td>\n",
       "      <td>-0.725446</td>\n",
       "      <td>-0.482330</td>\n",
       "      <td>-1.060820</td>\n",
       "      <td>-1.405190</td>\n",
       "      <td>-0.951023</td>\n",
       "      <td>-0.951023</td>\n",
       "      <td>1.540413</td>\n",
       "      <td>-0.983454</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  language  sentence  vertex   n    degree  closeness  harmonic  \\\n",
       "0   1  Japanese         1      38  43  0.043173   0.114668  0.270083   \n",
       "1   1  Japanese         1      33  43 -0.885052  -0.428544 -0.934764   \n",
       "2   1  Japanese         1      10  43  1.899625   0.766913  1.821878   \n",
       "3   1  Japanese         1      24  43  0.043173  -1.078191 -1.090006   \n",
       "4   1  Japanese         1      16  43 -0.885052  -1.405190 -1.871333   \n",
       "\n",
       "   betweeness      load  pagerank  eigenvector      katz  information  \\\n",
       "0   -0.710642 -0.710642  0.120017    -0.482073  0.031637     0.114668   \n",
       "1   -0.951023 -0.951023 -0.801517    -0.482239 -1.040168    -0.428544   \n",
       "2    0.743366  0.743366  1.764984    -0.481724  1.960983     0.766913   \n",
       "3   -0.710642 -0.710642  0.299057    -0.482315 -0.174878    -1.078191   \n",
       "4   -0.951023 -0.951023 -0.725446    -0.482330 -1.060820    -1.405190   \n",
       "\n",
       "   current_flow_betweeness  percolation  second_order  laplacian  \n",
       "0                -0.710642    -0.710642     -0.204072  -0.079853  \n",
       "1                -0.951023    -0.951023      0.347011  -0.983454  \n",
       "2                 0.743366     0.743366     -0.799779   1.727348  \n",
       "3                -0.710642    -0.710642      1.103667  -0.441293  \n",
       "4                -0.951023    -0.951023      1.540413  -0.983454  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('../data/normalized_expanded_test.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6233c00",
   "metadata": {},
   "source": [
    "Next we create a script that for each sample of the test data  (unique id), uses the corresponding language model to make the predictions (binary classification node level predictions). Then the final prediction of the root node of the sentence instance id is the node with the highest probability to belong to the class 1: `root` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f12c23bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for test_sample_id in test.id.unique():\n",
    "    current_sample = test[test.id == test_sample_id].copy()\n",
    "    language = current_sample.language.iloc[0]\n",
    "    # Create a new column storing the probabilities\n",
    "    current_sample['probabilities'] = best_models[language].predict_proba(current_sample[feature_cols])[:, 1]\n",
    "    # Returning the vertex number of the row witht the highest probabilities\n",
    "    predicted_root = current_sample.loc[current_sample['probabilities'].idxmax(), 'vertex']\n",
    "    results.append({'id':test_sample_id, 'root':predicted_root})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b408b0",
   "metadata": {},
   "source": [
    "Now we can create a dataframe from the results and save it as csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b958d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results_df = pd.DataFrame(results)\n",
    "final_results_df.to_csv('../data/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python ML-project(myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
