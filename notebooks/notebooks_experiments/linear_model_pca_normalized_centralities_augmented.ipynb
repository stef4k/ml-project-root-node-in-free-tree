{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a55429ae",
   "metadata": {},
   "source": [
    "# Linear Approach\n",
    "This attempt focuses on using normalized centrality measures and the average centrality measures of each node's neighbors, a PCA and logistic regression for each of the languages\n",
    "\n",
    "Starting by importing the right libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c508369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit, StratifiedGroupKFold\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import statistics\n",
    "import random\n",
    "import warnings\n",
    "from sklearn.exceptions import FitFailedWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FitFailedWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82005bf0",
   "metadata": {},
   "source": [
    "Next we read the train data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f0da078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>sentence</th>\n",
       "      <th>vertex</th>\n",
       "      <th>n</th>\n",
       "      <th>degree</th>\n",
       "      <th>closeness</th>\n",
       "      <th>harmonic</th>\n",
       "      <th>betweeness</th>\n",
       "      <th>load</th>\n",
       "      <th>pagerank</th>\n",
       "      <th>...</th>\n",
       "      <th>current_flow_betweeness_nhop1_sum</th>\n",
       "      <th>percolation_nhop1_mean</th>\n",
       "      <th>percolation_nhop1_max</th>\n",
       "      <th>percolation_nhop1_sum</th>\n",
       "      <th>second_order_nhop1_mean</th>\n",
       "      <th>second_order_nhop1_max</th>\n",
       "      <th>second_order_nhop1_sum</th>\n",
       "      <th>laplacian_nhop1_mean</th>\n",
       "      <th>laplacian_nhop1_max</th>\n",
       "      <th>laplacian_nhop1_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>0.133038</td>\n",
       "      <td>-0.558160</td>\n",
       "      <td>-0.308023</td>\n",
       "      <td>-0.676962</td>\n",
       "      <td>-0.676962</td>\n",
       "      <td>0.409538</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.961448</td>\n",
       "      <td>-0.861890</td>\n",
       "      <td>-0.537040</td>\n",
       "      <td>-0.961448</td>\n",
       "      <td>0.730613</td>\n",
       "      <td>1.021697</td>\n",
       "      <td>0.842049</td>\n",
       "      <td>-0.554857</td>\n",
       "      <td>0.758426</td>\n",
       "      <td>-0.578815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>-1.396899</td>\n",
       "      <td>-1.111953</td>\n",
       "      <td>-1.442852</td>\n",
       "      <td>-1.126101</td>\n",
       "      <td>-1.126101</td>\n",
       "      <td>-1.313638</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.755507</td>\n",
       "      <td>-1.094197</td>\n",
       "      <td>-1.427690</td>\n",
       "      <td>-0.755507</td>\n",
       "      <td>0.748143</td>\n",
       "      <td>0.276860</td>\n",
       "      <td>0.553739</td>\n",
       "      <td>-0.554857</td>\n",
       "      <td>-1.127390</td>\n",
       "      <td>-0.552609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>1.662975</td>\n",
       "      <td>0.111311</td>\n",
       "      <td>0.741550</td>\n",
       "      <td>0.135764</td>\n",
       "      <td>0.135764</td>\n",
       "      <td>1.885752</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.223610</td>\n",
       "      <td>-0.820895</td>\n",
       "      <td>-0.232344</td>\n",
       "      <td>-1.223610</td>\n",
       "      <td>0.230733</td>\n",
       "      <td>0.352194</td>\n",
       "      <td>0.378021</td>\n",
       "      <td>-0.732103</td>\n",
       "      <td>-0.184482</td>\n",
       "      <td>-0.906393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>-1.396899</td>\n",
       "      <td>-0.618060</td>\n",
       "      <td>-0.907167</td>\n",
       "      <td>-1.126101</td>\n",
       "      <td>-1.126101</td>\n",
       "      <td>-1.449223</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.221405</td>\n",
       "      <td>-0.055651</td>\n",
       "      <td>-0.537040</td>\n",
       "      <td>-0.221405</td>\n",
       "      <td>-0.098853</td>\n",
       "      <td>-0.500142</td>\n",
       "      <td>0.123792</td>\n",
       "      <td>1.572094</td>\n",
       "      <td>0.758426</td>\n",
       "      <td>0.652876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>0.133038</td>\n",
       "      <td>0.812630</td>\n",
       "      <td>0.880961</td>\n",
       "      <td>0.413803</td>\n",
       "      <td>0.413803</td>\n",
       "      <td>-0.074579</td>\n",
       "      <td>...</td>\n",
       "      <td>1.076574</td>\n",
       "      <td>1.119547</td>\n",
       "      <td>1.478643</td>\n",
       "      <td>1.076574</td>\n",
       "      <td>-0.906266</td>\n",
       "      <td>-0.500142</td>\n",
       "      <td>-0.819755</td>\n",
       "      <td>1.837963</td>\n",
       "      <td>1.229880</td>\n",
       "      <td>2.133527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   language  sentence  vertex   n    degree  closeness  harmonic  betweeness  \\\n",
       "0  Japanese         2       6  23  0.133038  -0.558160 -0.308023   -0.676962   \n",
       "1  Japanese         2       4  23 -1.396899  -1.111953 -1.442852   -1.126101   \n",
       "2  Japanese         2       2  23  1.662975   0.111311  0.741550    0.135764   \n",
       "3  Japanese         2      23  23 -1.396899  -0.618060 -0.907167   -1.126101   \n",
       "4  Japanese         2      20  23  0.133038   0.812630  0.880961    0.413803   \n",
       "\n",
       "       load  pagerank  ...  current_flow_betweeness_nhop1_sum  \\\n",
       "0 -0.676962  0.409538  ...                          -0.961448   \n",
       "1 -1.126101 -1.313638  ...                          -0.755507   \n",
       "2  0.135764  1.885752  ...                          -1.223610   \n",
       "3 -1.126101 -1.449223  ...                          -0.221405   \n",
       "4  0.413803 -0.074579  ...                           1.076574   \n",
       "\n",
       "   percolation_nhop1_mean  percolation_nhop1_max  percolation_nhop1_sum  \\\n",
       "0               -0.861890              -0.537040              -0.961448   \n",
       "1               -1.094197              -1.427690              -0.755507   \n",
       "2               -0.820895              -0.232344              -1.223610   \n",
       "3               -0.055651              -0.537040              -0.221405   \n",
       "4                1.119547               1.478643               1.076574   \n",
       "\n",
       "   second_order_nhop1_mean  second_order_nhop1_max  second_order_nhop1_sum  \\\n",
       "0                 0.730613                1.021697                0.842049   \n",
       "1                 0.748143                0.276860                0.553739   \n",
       "2                 0.230733                0.352194                0.378021   \n",
       "3                -0.098853               -0.500142                0.123792   \n",
       "4                -0.906266               -0.500142               -0.819755   \n",
       "\n",
       "   laplacian_nhop1_mean  laplacian_nhop1_max  laplacian_nhop1_sum  \n",
       "0             -0.554857             0.758426            -0.578815  \n",
       "1             -0.554857            -1.127390            -0.552609  \n",
       "2             -0.732103            -0.184482            -0.906393  \n",
       "3              1.572094             0.758426             0.652876  \n",
       "4              1.837963             1.229880             2.133527  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../../data/normalized_augmented_train_1hop_allfeatures.csv')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e921a9ca",
   "metadata": {},
   "source": [
    "Now we will focus only in one language, opting to go for `Polish`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83483938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>sentence</th>\n",
       "      <th>vertex</th>\n",
       "      <th>n</th>\n",
       "      <th>degree</th>\n",
       "      <th>closeness</th>\n",
       "      <th>harmonic</th>\n",
       "      <th>betweeness</th>\n",
       "      <th>load</th>\n",
       "      <th>pagerank</th>\n",
       "      <th>...</th>\n",
       "      <th>current_flow_betweeness_nhop1_sum</th>\n",
       "      <th>percolation_nhop1_mean</th>\n",
       "      <th>percolation_nhop1_max</th>\n",
       "      <th>percolation_nhop1_sum</th>\n",
       "      <th>second_order_nhop1_mean</th>\n",
       "      <th>second_order_nhop1_max</th>\n",
       "      <th>second_order_nhop1_sum</th>\n",
       "      <th>laplacian_nhop1_mean</th>\n",
       "      <th>laplacian_nhop1_max</th>\n",
       "      <th>laplacian_nhop1_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>126122</th>\n",
       "      <td>Polish</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>-0.983415</td>\n",
       "      <td>-1.112357</td>\n",
       "      <td>-1.140267</td>\n",
       "      <td>-0.862200</td>\n",
       "      <td>-0.862200</td>\n",
       "      <td>-0.946343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.673061</td>\n",
       "      <td>-0.659982</td>\n",
       "      <td>-1.144528</td>\n",
       "      <td>-0.673061</td>\n",
       "      <td>0.733398</td>\n",
       "      <td>0.261813</td>\n",
       "      <td>0.570124</td>\n",
       "      <td>-0.037514</td>\n",
       "      <td>-0.671478</td>\n",
       "      <td>-0.291875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130297</th>\n",
       "      <td>Polish</td>\n",
       "      <td>567</td>\n",
       "      <td>25</td>\n",
       "      <td>35</td>\n",
       "      <td>-0.931149</td>\n",
       "      <td>-1.425134</td>\n",
       "      <td>-1.327955</td>\n",
       "      <td>-0.808350</td>\n",
       "      <td>-0.808350</td>\n",
       "      <td>-0.898689</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.837453</td>\n",
       "      <td>-0.868841</td>\n",
       "      <td>-1.219894</td>\n",
       "      <td>-0.837453</td>\n",
       "      <td>1.273974</td>\n",
       "      <td>0.900006</td>\n",
       "      <td>0.774337</td>\n",
       "      <td>-0.056030</td>\n",
       "      <td>-0.548010</td>\n",
       "      <td>-0.310756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128519</th>\n",
       "      <td>Polish</td>\n",
       "      <td>357</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.816497</td>\n",
       "      <td>-0.465846</td>\n",
       "      <td>-0.528847</td>\n",
       "      <td>-0.836117</td>\n",
       "      <td>-0.836117</td>\n",
       "      <td>-0.841449</td>\n",
       "      <td>...</td>\n",
       "      <td>0.586736</td>\n",
       "      <td>1.267236</td>\n",
       "      <td>0.274625</td>\n",
       "      <td>0.586736</td>\n",
       "      <td>-1.190242</td>\n",
       "      <td>-1.264715</td>\n",
       "      <td>-0.493920</td>\n",
       "      <td>1.424305</td>\n",
       "      <td>0.857505</td>\n",
       "      <td>1.044150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132310</th>\n",
       "      <td>Polish</td>\n",
       "      <td>825</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>2.083023</td>\n",
       "      <td>0.453251</td>\n",
       "      <td>1.350470</td>\n",
       "      <td>1.156678</td>\n",
       "      <td>1.156678</td>\n",
       "      <td>2.162341</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.650223</td>\n",
       "      <td>-1.204552</td>\n",
       "      <td>0.113984</td>\n",
       "      <td>-1.650223</td>\n",
       "      <td>0.788028</td>\n",
       "      <td>0.734528</td>\n",
       "      <td>1.114068</td>\n",
       "      <td>-1.159880</td>\n",
       "      <td>-1.236159</td>\n",
       "      <td>-1.813025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129666</th>\n",
       "      <td>Polish</td>\n",
       "      <td>490</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>0.068199</td>\n",
       "      <td>0.728343</td>\n",
       "      <td>0.712585</td>\n",
       "      <td>0.420129</td>\n",
       "      <td>0.420129</td>\n",
       "      <td>-0.047339</td>\n",
       "      <td>...</td>\n",
       "      <td>1.228535</td>\n",
       "      <td>0.913466</td>\n",
       "      <td>1.084363</td>\n",
       "      <td>1.228535</td>\n",
       "      <td>-0.607010</td>\n",
       "      <td>-0.091420</td>\n",
       "      <td>-0.535908</td>\n",
       "      <td>0.897904</td>\n",
       "      <td>1.192677</td>\n",
       "      <td>1.094490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       language  sentence  vertex   n    degree  closeness  harmonic  \\\n",
       "126122   Polish        46       2  23 -0.983415  -1.112357 -1.140267   \n",
       "130297   Polish       567      25  35 -0.931149  -1.425134 -1.327955   \n",
       "128519   Polish       357       9  10 -0.816497  -0.465846 -0.528847   \n",
       "132310   Polish       825      10  15  2.083023   0.453251  1.350470   \n",
       "129666   Polish       490      26  27  0.068199   0.728343  0.712585   \n",
       "\n",
       "        betweeness      load  pagerank  ...  \\\n",
       "126122   -0.862200 -0.862200 -0.946343  ...   \n",
       "130297   -0.808350 -0.808350 -0.898689  ...   \n",
       "128519   -0.836117 -0.836117 -0.841449  ...   \n",
       "132310    1.156678  1.156678  2.162341  ...   \n",
       "129666    0.420129  0.420129 -0.047339  ...   \n",
       "\n",
       "        current_flow_betweeness_nhop1_sum  percolation_nhop1_mean  \\\n",
       "126122                          -0.673061               -0.659982   \n",
       "130297                          -0.837453               -0.868841   \n",
       "128519                           0.586736                1.267236   \n",
       "132310                          -1.650223               -1.204552   \n",
       "129666                           1.228535                0.913466   \n",
       "\n",
       "        percolation_nhop1_max  percolation_nhop1_sum  second_order_nhop1_mean  \\\n",
       "126122              -1.144528              -0.673061                 0.733398   \n",
       "130297              -1.219894              -0.837453                 1.273974   \n",
       "128519               0.274625               0.586736                -1.190242   \n",
       "132310               0.113984              -1.650223                 0.788028   \n",
       "129666               1.084363               1.228535                -0.607010   \n",
       "\n",
       "        second_order_nhop1_max  second_order_nhop1_sum  laplacian_nhop1_mean  \\\n",
       "126122                0.261813                0.570124             -0.037514   \n",
       "130297                0.900006                0.774337             -0.056030   \n",
       "128519               -1.264715               -0.493920              1.424305   \n",
       "132310                0.734528                1.114068             -1.159880   \n",
       "129666               -0.091420               -0.535908              0.897904   \n",
       "\n",
       "        laplacian_nhop1_max  laplacian_nhop1_sum  \n",
       "126122            -0.671478            -0.291875  \n",
       "130297            -0.548010            -0.310756  \n",
       "128519             0.857505             1.044150  \n",
       "132310            -1.236159            -1.813025  \n",
       "129666             1.192677             1.094490  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polish_data = data[data.language == 'Polish'].sample(frac=1, random_state=1).copy()\n",
    "polish_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190efb91",
   "metadata": {},
   "source": [
    "Now we can use pca to reduce the centrality dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "141985d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=7)\n",
    "feature_cols = ['degree', 'closeness','harmonic', 'betweeness', 'load', 'pagerank', 'eigenvector', 'katz',\n",
    "       'information', 'current_flow_betweeness', 'percolation', 'second_order',\n",
    "       'laplacian', 'degree_nhop1_mean', 'degree_nhop1_max',\n",
    "       'degree_nhop1_sum', 'closeness_nhop1_mean', 'closeness_nhop1_max',\n",
    "       'closeness_nhop1_sum', 'harmonic_nhop1_mean', 'harmonic_nhop1_max',\n",
    "       'harmonic_nhop1_sum', 'betweeness_nhop1_mean', 'betweeness_nhop1_max',\n",
    "       'betweeness_nhop1_sum', 'load_nhop1_mean', 'load_nhop1_max', 'load_nhop1_sum', 'pagerank_nhop1_mean', 'pagerank_nhop1_max',\n",
    "       'pagerank_nhop1_sum', 'eigenvector_nhop1_mean', 'eigenvector_nhop1_max',\n",
    "       'eigenvector_nhop1_sum', 'katz_nhop1_mean', 'katz_nhop1_max',\n",
    "       'katz_nhop1_sum', 'information_nhop1_mean', 'information_nhop1_max',\n",
    "       'information_nhop1_sum', 'current_flow_betweeness_nhop1_mean',\n",
    "       'current_flow_betweeness_nhop1_max','current_flow_betweeness_nhop1_sum', 'percolation_nhop1_mean',\n",
    "       'percolation_nhop1_max', 'percolation_nhop1_sum','second_order_nhop1_mean', 'second_order_nhop1_max',\n",
    "       'second_order_nhop1_sum', 'laplacian_nhop1_mean', 'laplacian_nhop1_max',\n",
    "       'laplacian_nhop1_sum']\n",
    "X_pca = pca.fit_transform(polish_data[feature_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec374cae",
   "metadata": {},
   "source": [
    "The explained variance reaches almost 100% by just using 7 principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd3948e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9844359775087458)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f4a227",
   "metadata": {},
   "source": [
    "Now we can split the data in train and test in a way to keep the same sentences (sentence level and node level of sentence) in the same group. Each sentence and its nodes will only be found in train or validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7693d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = polish_data.copy()#.drop(['is_root','language','n'], axis=1).copy()\n",
    "y = polish_data['is_root'].copy()\n",
    "\n",
    "groups = polish_data['sentence']\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "X_train, X_val= X.iloc[train_idx], X.iloc[val_idx]\n",
    "y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c84185",
   "metadata": {},
   "source": [
    "Now we also define the cross validation strategy. We also split the training data in 5 different folds ensuring that data of of the same sentence can not be found in different folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c45bd94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedGroupKFold(n_splits=5)\n",
    "groups = X_train['sentence']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaec98e",
   "metadata": {},
   "source": [
    "Next we can define the pipeline of pca and logistic regression model parameters that we want to try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af856faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns\n",
    "feature_cols = ['sentence', 'degree', 'closeness','harmonic', 'betweeness', 'load', 'pagerank', 'eigenvector', 'katz',\n",
    "       'information', 'current_flow_betweeness', 'percolation', 'second_order',\n",
    "       'laplacian', 'degree_nhop1_mean', 'degree_nhop1_max',\n",
    "       'degree_nhop1_sum', 'closeness_nhop1_mean', 'closeness_nhop1_max',\n",
    "       'closeness_nhop1_sum', 'harmonic_nhop1_mean', 'harmonic_nhop1_max',\n",
    "       'harmonic_nhop1_sum', 'betweeness_nhop1_mean', 'betweeness_nhop1_max',\n",
    "       'betweeness_nhop1_sum', 'load_nhop1_mean', 'load_nhop1_max', 'load_nhop1_sum', 'pagerank_nhop1_mean', 'pagerank_nhop1_max',\n",
    "       'pagerank_nhop1_sum', 'eigenvector_nhop1_mean', 'eigenvector_nhop1_max',\n",
    "       'eigenvector_nhop1_sum', 'katz_nhop1_mean', 'katz_nhop1_max',\n",
    "       'katz_nhop1_sum', 'information_nhop1_mean', 'information_nhop1_max',\n",
    "       'information_nhop1_sum', 'current_flow_betweeness_nhop1_mean',\n",
    "       'current_flow_betweeness_nhop1_max','current_flow_betweeness_nhop1_sum', 'percolation_nhop1_mean',\n",
    "       'percolation_nhop1_max', 'percolation_nhop1_sum','second_order_nhop1_mean', 'second_order_nhop1_max',\n",
    "       'second_order_nhop1_sum', 'laplacian_nhop1_mean', 'laplacian_nhop1_max',\n",
    "       'laplacian_nhop1_sum']\n",
    "\n",
    "pca_columns = X_train[feature_cols].columns.difference(['sentence'])\n",
    "passthrough_columns = ['sentence']\n",
    "\n",
    "# We want the sentence column to passthrough and not be transformed since it is used for the scoring\n",
    "preprocessor = ColumnTransformer([\n",
    "     ('pca_pipeline', Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=7))\n",
    "    ]), pca_columns),\n",
    "    ('passthrough', 'passthrough', passthrough_columns)\n",
    "])\n",
    "\n",
    "# Full pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=2, n_jobs=7))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "    'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
    "    'classifier__solver': ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
    "    'classifier__l1_ratio': [0, 0.5, 1],\n",
    "    'classifier__fit_intercept':[True,False],\n",
    "    'classifier__warm_start': [True,False],\n",
    "    'classifier__max_iter': [100, 250, 500, 1000, 2500, 5000]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc584422",
   "metadata": {},
   "source": [
    "We use a custom scoring method that picks one root per sentence. For each sentence, it chooses the node with the highest chance of being the root (class 1). The final score is just the percentage of sentences where we picked the correct root.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bbde6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_prediction_score(estimator, X, y_true):\n",
    "    \"\"\"\n",
    "    Scoring function that extracts sentence IDs from X and computes\n",
    "    root prediction accuracy per sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    sentence_ids = X['sentence'].values\n",
    "    X_features = X.copy()\n",
    "    # Predict probabilities\n",
    "    probs = estimator.predict_proba(X_features)[:, 1]\n",
    "    # Build DataFrame for groupby\n",
    "    df_pred = pd.DataFrame({\n",
    "        'sentence': sentence_ids,\n",
    "        'is_root': y_true,\n",
    "        'root_prob': probs\n",
    "    })\n",
    "\n",
    "    predicted_roots = df_pred.loc[df_pred.groupby('sentence')['root_prob'].idxmax()]\n",
    "    accuracy = float((predicted_roots['is_root'] == 1).mean())\n",
    "    return accuracy\n",
    "\n",
    "root_scorer = make_scorer(root_prediction_score, greater_is_better=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b790f57",
   "metadata": {},
   "source": [
    "Now we can do the gridsearch, combining defined parameters, the custom scoring function, the cross validation strategy (that ensures no data leakage):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aae5bce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'classifier__warm_start': False, 'classifier__solver': 'lbfgs', 'classifier__penalty': None, 'classifier__max_iter': 100, 'classifier__l1_ratio': 1, 'classifier__fit_intercept': True, 'classifier__C': 0.01}\n",
      "Best score: 0.28500000000000003\n"
     ]
    }
   ],
   "source": [
    "\n",
    "grid_search = RandomizedSearchCV(n_iter=500,\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_grid,\n",
    "    cv=cv,\n",
    "    scoring=root_prediction_score,\n",
    "    n_jobs=7,\n",
    "    random_state=2\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train[feature_cols], y_train, groups=groups)\n",
    "\n",
    "print(\"Best params:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104a3cd3",
   "metadata": {},
   "source": [
    "We will use the best estimator found to see the performance in the validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df451647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_estimator = grid_search.best_estimator_\n",
    "root_prediction_score(best_estimator, X_val[feature_cols], y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b346f0ab",
   "metadata": {},
   "source": [
    "### Extending all languages\n",
    "Next we create a script that follows the previous steps for each of the 21 languages. As a result we will have one logistic regression model for each language. To predict the test data, we will use the particular logistic regression depending on the language of the row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce52cf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Japanese\n",
      "Grid search score: 0.095\n",
      "Validation score: 0.09\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 5, 'classifier__warm_start': True, 'classifier__solver': 'newton-cg', 'classifier__penalty': None, 'classifier__max_iter': 500, 'classifier__l1_ratio': 1, 'classifier__fit_intercept': True, 'classifier__C': 0.1}\n",
      "---------------------------------------\n",
      "Finnish\n",
      "Grid search score: 0.355\n",
      "Validation score: 0.35\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 9, 'classifier__warm_start': True, 'classifier__solver': 'lbfgs', 'classifier__penalty': None, 'classifier__max_iter': 250, 'classifier__l1_ratio': 0.5, 'classifier__fit_intercept': True, 'classifier__C': 0.1}\n",
      "---------------------------------------\n",
      "Galician\n",
      "Grid search score: 0.29250000000000004\n",
      "Validation score: 0.32\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 9, 'classifier__warm_start': True, 'classifier__solver': 'newton-cholesky', 'classifier__penalty': None, 'classifier__max_iter': 500, 'classifier__l1_ratio': 1, 'classifier__fit_intercept': True, 'classifier__C': 10}\n",
      "---------------------------------------\n",
      "English\n",
      "Grid search score: 0.3\n",
      "Validation score: 0.28\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 9, 'classifier__warm_start': True, 'classifier__solver': 'liblinear', 'classifier__penalty': 'l1', 'classifier__max_iter': 2000, 'classifier__l1_ratio': 0, 'classifier__fit_intercept': True, 'classifier__C': 100}\n",
      "---------------------------------------\n",
      "Hindi\n",
      "Grid search score: 0.22000000000000003\n",
      "Validation score: 0.26\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 9, 'classifier__warm_start': False, 'classifier__solver': 'liblinear', 'classifier__penalty': 'l1', 'classifier__max_iter': 750, 'classifier__l1_ratio': 1, 'classifier__fit_intercept': True, 'classifier__C': 100}\n",
      "---------------------------------------\n",
      "French\n",
      "Grid search score: 0.275\n",
      "Validation score: 0.28\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 9, 'classifier__warm_start': False, 'classifier__solver': 'lbfgs', 'classifier__penalty': 'l2', 'classifier__max_iter': 250, 'classifier__l1_ratio': 0, 'classifier__fit_intercept': True, 'classifier__C': 10}\n",
      "---------------------------------------\n",
      "Italian\n",
      "Grid search score: 0.27749999999999997\n",
      "Validation score: 0.26\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 9, 'classifier__warm_start': False, 'classifier__solver': 'newton-cg', 'classifier__penalty': 'l2', 'classifier__max_iter': 500, 'classifier__l1_ratio': 0.5, 'classifier__fit_intercept': True, 'classifier__C': 1}\n",
      "---------------------------------------\n",
      "Indonesian\n",
      "Grid search score: 0.3125\n",
      "Validation score: 0.3\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 9, 'classifier__warm_start': True, 'classifier__solver': 'newton-cholesky', 'classifier__penalty': None, 'classifier__max_iter': 1000, 'classifier__l1_ratio': 0, 'classifier__fit_intercept': True, 'classifier__C': 0.1}\n",
      "---------------------------------------\n",
      "Swedish\n",
      "Grid search score: 0.3175\n",
      "Validation score: 0.3\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 9, 'classifier__warm_start': True, 'classifier__solver': 'newton-cholesky', 'classifier__penalty': None, 'classifier__max_iter': 250, 'classifier__l1_ratio': 0.5, 'classifier__fit_intercept': True, 'classifier__C': 0.01}\n",
      "---------------------------------------\n",
      "Spanish\n",
      "Grid search score: 0.3175\n",
      "Validation score: 0.3\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 9, 'classifier__warm_start': False, 'classifier__solver': 'newton-cg', 'classifier__penalty': None, 'classifier__max_iter': 1000, 'classifier__l1_ratio': 0, 'classifier__fit_intercept': False, 'classifier__C': 1}\n",
      "---------------------------------------\n",
      "Icelandic\n",
      "Grid search score: 0.335\n",
      "Validation score: 0.34\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 9, 'classifier__warm_start': False, 'classifier__solver': 'liblinear', 'classifier__penalty': 'l2', 'classifier__max_iter': 50, 'classifier__l1_ratio': 0, 'classifier__fit_intercept': False, 'classifier__C': 10}\n",
      "---------------------------------------\n",
      "German\n",
      "Grid search score: 0.27499999999999997\n",
      "Validation score: 0.31\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 8, 'classifier__warm_start': True, 'classifier__solver': 'newton-cg', 'classifier__penalty': None, 'classifier__max_iter': 500, 'classifier__l1_ratio': 0.5, 'classifier__fit_intercept': False, 'classifier__C': 100}\n",
      "---------------------------------------\n",
      "Korean\n",
      "Grid search score: 0.315\n",
      "Validation score: 0.33\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 5, 'classifier__warm_start': False, 'classifier__solver': 'liblinear', 'classifier__penalty': 'l2', 'classifier__max_iter': 500, 'classifier__l1_ratio': 0, 'classifier__fit_intercept': False, 'classifier__C': 1}\n",
      "---------------------------------------\n",
      "Polish\n",
      "Grid search score: 0.27499999999999997\n",
      "Validation score: 0.34\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 8, 'classifier__warm_start': False, 'classifier__solver': 'saga', 'classifier__penalty': 'elasticnet', 'classifier__max_iter': 100, 'classifier__l1_ratio': 0.5, 'classifier__fit_intercept': False, 'classifier__C': 0.01}\n",
      "---------------------------------------\n",
      "Thai\n",
      "Grid search score: 0.25\n",
      "Validation score: 0.28\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 5, 'classifier__warm_start': True, 'classifier__solver': 'liblinear', 'classifier__penalty': 'l2', 'classifier__max_iter': 50, 'classifier__l1_ratio': 0, 'classifier__fit_intercept': True, 'classifier__C': 10}\n",
      "---------------------------------------\n",
      "Turkish\n",
      "Grid search score: 0.30250000000000005\n",
      "Validation score: 0.34\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 6, 'classifier__warm_start': False, 'classifier__solver': 'saga', 'classifier__penalty': 'l2', 'classifier__max_iter': 1000, 'classifier__l1_ratio': 1, 'classifier__fit_intercept': True, 'classifier__C': 10}\n",
      "---------------------------------------\n",
      "Czech\n",
      "Grid search score: 0.2725\n",
      "Validation score: 0.33\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 9, 'classifier__warm_start': True, 'classifier__solver': 'liblinear', 'classifier__penalty': 'l1', 'classifier__max_iter': 750, 'classifier__l1_ratio': 1, 'classifier__fit_intercept': True, 'classifier__C': 0.1}\n",
      "---------------------------------------\n",
      "Chinese\n",
      "Grid search score: 0.2975\n",
      "Validation score: 0.26\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 9, 'classifier__warm_start': True, 'classifier__solver': 'newton-cg', 'classifier__penalty': None, 'classifier__max_iter': 50, 'classifier__l1_ratio': 1, 'classifier__fit_intercept': True, 'classifier__C': 1}\n",
      "---------------------------------------\n",
      "Portuguese\n",
      "Grid search score: 0.3325\n",
      "Validation score: 0.31\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 9, 'classifier__warm_start': True, 'classifier__solver': 'liblinear', 'classifier__penalty': 'l1', 'classifier__max_iter': 500, 'classifier__l1_ratio': 0.5, 'classifier__fit_intercept': False, 'classifier__C': 100}\n",
      "---------------------------------------\n",
      "Arabic\n",
      "Grid search score: 0.2875\n",
      "Validation score: 0.32\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 5, 'classifier__warm_start': True, 'classifier__solver': 'newton-cholesky', 'classifier__penalty': 'l2', 'classifier__max_iter': 500, 'classifier__l1_ratio': 0, 'classifier__fit_intercept': True, 'classifier__C': 10}\n",
      "---------------------------------------\n",
      "Russian\n",
      "Grid search score: 0.35250000000000004\n",
      "Validation score: 0.31\n",
      "{'preprocessor__pca_pipeline__pca__n_components': 6, 'classifier__warm_start': False, 'classifier__solver': 'lbfgs', 'classifier__penalty': 'l2', 'classifier__max_iter': 1000, 'classifier__l1_ratio': 0, 'classifier__fit_intercept': True, 'classifier__C': 0.1}\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "score = []\n",
    "best_models = {}\n",
    "random.seed(42)\n",
    "for language in data.language.unique():\n",
    "    X = data[data.language == language].sample(frac=1, random_state=1).copy()\n",
    "    y = X['is_root'].copy()\n",
    "\n",
    "    groups = X['sentence']\n",
    "\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    train_idx, val_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "    X_train, X_val= X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    cv = StratifiedGroupKFold(n_splits=5)\n",
    "    groups = X_train['sentence']\n",
    "    \n",
    "    feature_cols = [\n",
    "    'sentence', 'degree', 'closeness', 'harmonic', 'betweeness', 'load', 'pagerank',\n",
    "    'eigenvector', 'katz', 'information', 'current_flow_betweeness',\n",
    "    'percolation', 'second_order', 'laplacian'\n",
    "    ]\n",
    "    pca_columns = X_train[feature_cols].columns.difference(['sentence'])\n",
    "    passthrough_columns = ['sentence']\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('pca_pipeline', Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('pca', PCA())\n",
    "        ]), pca_columns),\n",
    "        ('passthrough', 'passthrough', passthrough_columns)\n",
    "    ])\n",
    "\n",
    "    # Final pipeline of pca and classifier\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', LogisticRegression(random_state=2, n_jobs=7))\n",
    "    ])\n",
    "\n",
    "    param_grid = {\n",
    "        'preprocessor__pca_pipeline__pca__n_components': [5,6,7,8,9],\n",
    "        'classifier__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "        'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'classifier__solver': ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
    "        'classifier__l1_ratio': [0, 0.5, 1],\n",
    "        'classifier__fit_intercept':[True,False],\n",
    "        'classifier__warm_start': [True,False],\n",
    "        'classifier__max_iter': [50, 100, 250, 500, 750, 1000, 2000]\n",
    "    }\n",
    "\n",
    "    grid_search = RandomizedSearchCV(n_iter=500,\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_grid,\n",
    "    cv=cv,\n",
    "    scoring=root_prediction_score,\n",
    "    n_jobs=7,\n",
    "    # We want to get random combination of params for each different language\n",
    "    random_state=random.randint(1, 1000)\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train[feature_cols], y_train, groups=groups)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    current_score = root_prediction_score(grid_search.best_estimator_, X_val[feature_cols], y_val)\n",
    "    score.append(current_score)\n",
    "    print(language)\n",
    "    print('Grid search score: ' + str(grid_search.best_score_))\n",
    "    print('Validation score: ' + str(current_score))\n",
    "    print(best_params)\n",
    "    print('---------------------------------------')\n",
    "    \n",
    "    # Rebuild the whole pipeline using the best params\n",
    "    final_preprocessor = ColumnTransformer([\n",
    "        ('pca_pipeline', Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('pca', PCA(n_components=best_params['preprocessor__pca_pipeline__pca__n_components']))\n",
    "        ]), pca_columns),\n",
    "        ('passthrough', 'passthrough', passthrough_columns)\n",
    "    ])\n",
    "\n",
    "    # Handle optional params\n",
    "    classifier_params = {\n",
    "        key.split(\"classifier__\")[1]: value\n",
    "        for key, value in best_params.items()\n",
    "        if key.startswith(\"classifier__\")\n",
    "    }\n",
    "\n",
    "    final_model = Pipeline([\n",
    "        ('preprocessor', final_preprocessor),\n",
    "        ('classifier', LogisticRegression(\n",
    "            **classifier_params,\n",
    "            n_jobs=7,\n",
    "            random_state=2\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "\n",
    "    final_model.fit(X[feature_cols], y)\n",
    "    best_models[language] = final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf503aed",
   "metadata": {},
   "source": [
    "Printing the average score for the whole validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a8a3221e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2957142857142857"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics.mean(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python ML-project(myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
